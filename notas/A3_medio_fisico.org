#+SETUPFILE: ../setup_notas.org
#+TITLE: El medio físico y el almacenamiento

* El medio físico
# <<FS_FIS>>

A lo largo del presente texto, particularmente de los capítulos
\ref{DIR} y \ref{FS} y siguiendo las prácticas a que ha impuesto la
realidad de los últimos 40 años, el término genérico de
/disco/ se ha empleado prácticamente como sinónimo de /medio de
almacenamiento a largo plazo/.

En este apéndice se abordan en primer término las características
principales del medio aún prevalente, los discos duros magnéticos
rotativos, y una introducción a las diferencias que presentan respecto
a otros medios, como los discos ópticos y los de estado sólido, así
como las implicaciones que éstos tienen sobre el material presentado
en el capítulo \ref{FS}.

Cabe mencionar que la razón de separar este contenido hacia un
apéndice es que, si bien estas funciones resultan relevantes para los
sistemas operativos y estos cada vez más van asumiendo las funciones
que aquí serán descritas, estas comenzaron siendo implementadas por
hardware especializado; fue apenas hasta la aparición de los esquemas
de manejo avanzado de volúmenes (que serán cubiertos en la sección
\ref{FS_FIS_man_av_vol}) que entran al ámbito del sistema operativo.

** Discos magnéticos rotativos

El principal medio de almacenamiento empleado en los últimos 40 años
es el /disco magnético/. Hay dos tipos diferentes de disco, aunque la
lógica de su funcionamiento es la misma: Los /discos duros/ y los
/discos flexibles/ (o /floppies/).

La principal diferencia entre estos es que los primeros son
típicamente almacenamiento /interno/ en los equipos de cómputo, y los
segundos fueron pensados para ser almacenamiento /transportable/. Los
discos duros tienen mucha mayor capacidad y son mucho más rápidos,
pero a cambio de ello, son correspondientemente más sensibles a la
contaminación por partículas de polvo y a daños mecánicos, razón por
la cual hoy en día se venden, junto con el mecanismo lector e incluso
la electrónica de control, en empaque sellado.

Un disco flexible es una hoja de material plástico, muy similar al
empleado en las cintas magnéticas, resguardado por un estuche
plástico. Al insertarse el disco en la unidad lectora, esta lo hace
girar sujetándolo por el centro, y las cabezas lectoras (en un
principio una sola; posteriormente aparecieron las unidades de doble
cara, con dos cabezas lectoras) se deslizan por una ventana que tiene
el estuche.

La mayor parte de los discos flexibles presentaban velocidades de
rotación de entre 300 y 400 revoluciones por minuto — Presentaban,
pues, una /demora rotacional/ de entre 0.15 y 0.2 segundos. La /demora
rotacional/ es el tiempo que toma la cabeza lectora en volver a
posicionarse sobre un mismo sector del disco. (Ver figura
\ref{FS_FIS_disco_duro})

A lo largo de más de 20 años se presentaron muy diferentes formatos
físicos siguiendo esta misma lógica, designándose principalmente por
su tamaño (en pulgadas). La capacidad de los discos, claro está, fue
creciendo con el paso de los años — Esto explica la aparente
contradicción de que los discos (físicamente) más chicos tenían más
capacidad que los más grandes.

#+caption: Principales formatos de disco flexible que se popularizaron en el mercado
|-----------------------+-------------+---------------+--------------|
|                       |  8 pulgadas | 5.25 pulgadas | 3.5 pulgadas |
|-----------------------+-------------+---------------+--------------|
| Fecha de introducción |        1971 |          1976 |         1982 |
| Capacidad             | 150KB-1.2MB |   110KB-1.2MB | 264KB-2.88MB |
| Velocidad (kbit/s)    |          33 |       125-500 |     250-1000 |
| Pistas por pulgada    |          48 |         48-96 |          135 |
|-----------------------+-------------+---------------+--------------|

El nombre de /disco duro/ o /disco flexible/ se debe al medio empleado
para el almacenamiento de la información (y no a la rigidez de su
/estuche/, como mucha gente erróneamente cree): Mientras que los discos
flexibles emplean una hoja plástica flexible, los
discos duros son metálicos. Los discos están /permanentemente/
montados sobre un eje, lo que permite que tengan una velocidad de giro
entre 20 y 50 veces mayor que los discos flexibles — Entre 4,200 y
15,000 revoluciones por minuto (RPM), esto es, con una demora
rotacional de entre 2 y 7.14 milisegundos.

Además, a excepción de algunos modelos tempranos, los discos duros
constituyen un paquete cerrado y sellado que incluye las cabezas de
lectura y escritura, y toda la electrónica de control. Esto permite
que los discos duros tengan densidades de almacenamiento y velocidades
de transmisión muy superiores a la de los discos flexibles: Los
primeros discos duros que se comercializaron para computadoras
personales eran de 10MB (aproximadamente 70 discos flexibles de su
época), y actualmente hay ya discos de 4TB. La velocidad máxima de
transferencia sostenida hoy en día es superior a los 100MB por
segundo, 100 veces más rápido que la última generación de discos
flexibles.

Para medir la eficiencia de un disco duro, además de la /demora
rotacional/ presentada unos párrafos atrás, el otro dato importante es
el tiempo que toma la cabeza en moverse a través de la superficie del
disco. Hoy en día, las velocidades más comunes son de 20ms para un
/recorrido completo/ (desde el primer hasta el último sector), y entre
0.2ms y 0.8ms para ir de un cilindro al inmediato siguiente. Como
punto de comparación, el recorrido completo en una unidad de disco
flexible toma aproximadamente 100ms, y el tiempo de un cilindro al
siguiente va entre 3 y 8ms.

*** Notación C-H-S

En un principio y hasta la década de los noventa, el sistema operativo
siempre hacía referencia a la ubicación de un bloque de información en
el disco es conocido como la /notación C-H-S/ — Indicando el cilindro,
cabeza y sector (/Cylinder, Head, Sector/) para ubicar a cada bloque
de datos. Esto permite mapear el espacio de almacenamiento de un disco
a un espacio tridimensional, con cual resulta trivial ubicar a un
conjunto de datos en una región contigua.

#+attr_html: height="360" width="388"
#+attr_latex: width=0.5\textwidth
#+label: FS_FIS_disco_duro
#+caption: Coordenadas de un disco duro, ilustrando su geometría basada en cabeza, cilindro y sector. (Imagen de la Wikipedia: /Cilindro Cabeza Sector/)
[[./img/cilindro_cabeza_sector.png]]

La /cabeza/ indica a cuál de las superficies del disco se hace
referencia; en un disco flexible hay sólo una o dos cabezas (cuando
aparecieron las unidades de doble lado eran en un lujo, y al paso de
los años se fueron convirtiendo en la norma), pero en un disco duro es
común tener varios /platos/ paralelos. Todas las cabezas van fijas a
un mismo motor, por lo que no pueden moverse de forma independiente.

El /cilindro/ indica la distancia del centro a la orilla del disco. Al
cilindro también se le conoce como /pista/ (/track/), una metáfora
heredada de la época en que la música se distribuía principalmente en
discos de vinil, y se podía ver a simple vista la frontera entre una
pista y la siguiente.

Un /sector/ es un segmento de arco de uno de los cilindros, y contiene
siempre la misma cantidad de información (históricamente 512 bytes; en
actualmente se están adoptando gradualmente sectores de 4096
bytes. Refiérase a la sección \ref{FS_FIS_limitaciones} para una mayor
discusión al respecto.)

Un archivo almacenado secuencialmente ocupa /sectores adyacentes/ a lo
largo de una misma pista y con una misma cabeza.

*** Algoritmos de planificación de acceso a disco

Las transferencias desde y hacia los discos son uno de los procesos
más lentos de los que gestiona el sistema operativo. Cuando éste tiene
varias solicitudes de transferencia pendientes, resulta importante
encontrar un mecanismo óptimo para realizar la transferencia,
minimizando el tiempo de demora. A continuación se describirán a
grandes rasgos tres de los algoritmos históricos de planificación de
acceso a disco — Para abordar después el por qué estos hoy en día casi
no son empleados.

Como con los demás escenarios en que se han abordado algoritmos, para
analizar su rendimiento, el análisis se realizará sobre una /cadena de
referencia/. Este ejemplo supone un disco hipotético de 200 cilindros,
la cadena de solicitudes /83, 175, 40, 120, 15, 121, 41, 42/, y
teniendo la cabeza al inicio de la operación en el cilindro 60.

En la figura \ref{FS_FIS_mov_cabeza_por_algoritmo} puede apreciarse de
forma gráfica la respuesta que presentarían los distintos algoritmos
ante la cadena de referencia dada.


#+attr_latex: width=0.9\textwidth
#+label: FS_FIS_mov_cabeza_por_algoritmo
#+caption: Movimientos de las cabezas bajo los diferentes algoritmos planificadores de acceso a disco, indicando la distancia total recorrida por la cabeza bajo cada uno, iniciando con la cabeza en la posición 60. Para SCAN, LOOK y C-SCAN, se asume que la cabeza inicia avanzando en dirección decreciente.
[[./img/gnuplot/mov_cabeza_por_algoritmo.png]]

- FIFO :: Del mismo modo que cuando fueron presentados los algoritmos
          de asignación de procesador y de reemplazo de páginas, el
          primero y más sencillo de implementar es el /FIFO/ —
          /Primero llegado, primero servido/.

          Este algoritmo puede verse como muy
          /justo/, aunque sea muy poco eficiente: El movimiento total
          de cabezas para el caso planteado es de 622 cilindros,
          equivalente a poco más que recorrer de extremo a extremo el
          disco completo tres veces. Esto es, despreciando la demora
          rotacional la demora mecánica para que el brazo se detenga por
          completo antes de volver a moverse, esta lectura tomaría un
          mínimo de 60ms, siendo el recorrido completo del disco 20ms.

	  Puede identificarse como causante de buena parte de esta
          demora a la quinta posición de la cadena de referencia:
          Entre solicitudes para los cilindros contiguos 120 y 121,
          llegó una solicitud al 15.

	  Atender esta solicitud en FIFO significa un desplazamiento
          de $(120-15) + (121-15) = 211$ cilindros, para volver a
          quedar prácticamente en el mismo lugar de inicio. Una sola
          solicitud resulta responsable de la tercera parte del tiempo
          total.

- SSTF :: Ahora bien, si el factor que impone la principal demora es
          el movimiento de la cabeza, el segundo algoritmo busca
          reducir al mínimo el movimiento de la cabeza: /SSTF/
          (/Shortest Seek Time First/, /Tiempo de búsqueda más corto a
          continuación/) es el equivalente en este ámbito del
          /Proceso más corto a continuación/, presentado en la sección
          \ref{PLAN_spn} — con la ventaja de no estar prediciendo
          comportamiento futuro, sino partir de una lista de
          solicitudes pendientes. Empleando SSTF, el tiempo de
          desplazamiento para este caso se reduce a tan sólo 207
          cilindros, muy cerca del mínimo absoluto posible.

	  Una desventaja de SSTF es que puede llevar a la inanición:
          Si hay una gran densidad de solicitudes para cilindros en
          determinada zona del disco, una solicitud para un cilindro
          alejado puede quedar a la espera
          indefinidamente.

	  Ejemplificando esto con una serie de solicitudes distinta a
          la cadena referencia: Si el sistema tuviera que atender
          solicitudes por los cilindros /15, 175, 13, 20, 14, 32, 40,
          5, 6, 7/, SSTF /penalizaría/ a la segunda solicitud (175)
          hasta terminar con los cilindros bajos. Si durante el tiempo
          que tome responder a estas solicitudes llegan otras
          adicionales, el proceso que está esperando el contenido del
          cilindro 175 puede quedar en espera indefinida.

- Familia de algoritmos /de elevador/ (SCAN, LOOK, C-SCAN) :: En este
     tercer lugar se abordará ya no un sólo algoritmo, sino que una
     /familia/, dado que parten de la misma idea, pero con
     modificaciones menores llevan a que el patrón de atención
     resultante sea muy distinto.

     El planteamiento base para el algoritmo básico de elevador (SCAN)
     busca evitar la inanición, minimizando al mismo tiempo el
     movimiento de las cabezas. Su lógica indica que la cabeza debe
     recorrer el disco de extremo a extremo, como si fuera un elevador
     en un edificio alto, atendiendo a todas las solicitudes que haya
     pendientes en su camino. Si bien los recorridos para ciertos
     patrones pueden resultar en mayores desplazamientos a los que
     daría SSTF, la garantía de que ningún proceso esperará
     indefinidamente lo hace muy atractivo.

     Atender la cadena de referencia bajo SCAN, asumiendo un estado
     inicial /descendente/ (esto es, la cabeza está en el cilindro 60
     y va bajando) da un recorrido total de 235 cilindros; empleando
     LOOK, se reduce a 205 cilindros, y evita el movimiento
     innecesario hasta el límite del disco.

     Una primer (y casi obvia) modificación a este algoritmo sería,
     cada vez que la cabeza se detenga para satisfacer una solicitud,
     verificar si hay alguna otra solicitud pendiente en la /dirección
     actual/, y de no ser así, emprender el camino de regreso sin
     llegar a la orilla del disco. Esta modificación es frecuentemente
     descrita como /LOOK/.

     Sin embargo, el patrón de atención a solicitudes de SCAN y LOOK
     dejan qué desear: Al llegar a un extremo del recorrido, es
     bastante probable que no haya ninguna solicitud pendiente en la
     primer mitad del recorrido de vuelta (dado que acaban de ser
     atendidas). El tiempo que demora atender a una solictud se
     compone de la suma del desplazamiento de la cabeza y la demora
     rotacional (que depende de cuál sector del cilindro fue
     solicitado). Para mantener una tasa de transferencia más
     predecible, el algoritmo /C-SCAN/ (SCAN Circular) realiza las
     operaciones en el disco únicamente en un sentido — Si el
     algoritmo lee en órden /descendente/, al llegar a la solicitud
     del cilindro más bajo, saltará de vuelta hasta el más alto para
     volver a iniciar desde ahí. Esto tiene como resultado, claro, que
     el recorrido total aumente (aumentando hasta los 339 para la
     cadena de referencia presentada).

*** Limitaciones de los algoritmos presentados
# <<FS_FIS_limitaciones>>

Ahora bien, ¿por qué se mencionó que estos algoritmos hoy en día ya
casi no se usan?

Hay varias razones. En primer término, todos estos algoritmos
están orientados a reducir el traslado /de la cabeza/, pero ignoran
la /demora rotacional/. Como se explicó, en los discos duros actuales, la
demora rotacional va entre $1 \over 10$ y $1 \over 3$ del tiempo
total de recorrido de la cabeza. Y si bien el sistema podría
considerar esta demora como un factor adicional al planificar el
siguiente movimiento de forma que se redujera el tiempo de espera,
los algoritmos descritos obviamente requieren ser replanteados por
completo.

Por otro lado, el sistema operativo muchas veces requiere dar
distintas prioridades a los diferentes tipos de solicitud. Por
ejemplo, sería esperable que diera preferencia a los accesos a memoria
virtual por encima de las solicitudes de abrir un nuevo archivo. Estos
algoritmos tampoco permiten expresar esta necesidad.

Pero el tercer punto es mucho más importante aún: Del mismo modo
que los procesadores se van haciendo más rápidos y que la memoria
es cada vez de mayor capacidad, los controladores de discos también
son cada vez más /inteligentes/, y /esconden/ cada vez más
información del sistema operativo, por lo cual éste cada vez más
carece de la información necesaria acerca del acomodo /real/ de la
información como para planificar correctamente sus accesos.

Uno de los cambios más importantes en este sentido fue la transición
del empleo de la notación C-H-S al esquema de /direccionamiento lógico
de bloques/ (/Logical Block Addressing/, /LBA/) a principios de los
noventa. Hasta ese momento, el sistema operativo tenía información de
la ubicación /física/ de todos los bloques en el disco.

Una de las desventajas, sin embargo, de este esquema es que el mismo
BIOS tenía que conocer la /geometría/ de los discos — Y el BIOS
presentaba límites duros en este sentido: Principalmente, no le era
posible referenciar más allá de 64 cilindros. Al aparecer la interfaz
de discos IDE (/Electrónica integrada al dispositivo/) e ir
reemplazando a la ST-506, se introdujo LBA.

Este mecanismo convierte la dirección C-H-S a una dirección /lineal/,
presentando el disco al sistema operativo ya no como un espacio
/tridimensional/, sino que como un gran arreglo de bloques. En este
primer momento, partiendo de que $CPP$ denota el número de cabezas por
cilindro y $SPP$ el número de sectores por pista, la equivalencia de
una dirección C-H-S a una LBA era:

#+BEGIN_QUOTE
$LBA = ((C \times CPC) + H) \times SPP + S - 1$
#+END_QUOTE

LBA significó mucho más que una nueva notación: marcó el inicio de
la transferencia de inteligencia y control del CPU al controlador de
disco. El impacto de esto se refleja directamente en dos factores:

- Sectores variables por cilindro :: En casi todos los discos previos
     a LBA,[fn:: Las unidades de disco /Commodore 1541/ y /Macintosh
     Superdrive/, que empleaban velocidad variable por cilindro para
     aprovechar mejor el medio magnético, constituyen notorias
     excepciones; en ambos casos, sin embargo, terminaron
     desapareciendo por cuestiones de costos y de complejidad al
     sistema.] el número de sectores por pista se mantenía constante,
     se tratara de las pistas más internas o más externas. Esto
     significa que, a igual calidad de la cobertura magnética del
     medio, los sectores ubicados en la parte exterior del disco
     desperdiciaban mucho espacio (ya que el /área por bit/ era mucho
     mayor).

     #+attr_html: width="402" height="402"
     #+attr_latex: width=0.5\textwidth
     #+label: FS_FIS_zone_bit_recording
     #+caption: Disco formateado bajo /densidad de bits por zona/, con más sectores por pista en las pistas exteriores. (Imagen: Wikipedia)
     [[./img/zone_bit_recording.png]]

     Bajo LBA, los discos duros comenzaron a emplear un esquema de
     /densidad de bits por zona/ (/zone bit recording/), con la que en
     los cilindros más externos se aumenta.

- Reubicación de sectores :: Conforme avanza el uso de un disco, es
     posible que algunos sectores vayan resultando /difíciles/ de
     leer por daños microscópicos a la superficie. El controlador es
     capaz de detectar estos problemas, y de hecho, casi siempre
     puede rescatar la información de dichos sectores de forma
     imperceptible al usuario.

     Los discos duros ST-506 típicamente iban acompañados por una
     /lista de defectos/, una lista de coordenadas C-H-S que desde su
     fabricación habían presentado errores. El usuario debía ingresar
     estos defectos al formatear el disco /a bajo nivel/.

     Hoy en día, el controlador del disco detecta estos fallos y se
     los /salta/, presentando un mapa LBA lineal y completo. Los
     discos duros típicamente vienen con cierto número de /sectores de
     reserva/ para que, conforme se van detectando potenciales daños,
     estos puedan reemplazarse de forma transparente.

A estos factores se suma que a los controladores de disco se les
agregó también una memoria caché dedicada
para las operaciones de lectura y escritura. El controlador del disco
es hoy en día capaz de implementar estos mismos algoritmos de forma
completamente autónoma del sistema operativo.

Y si bien las diferentes unidades de disco duro habían mantenido
sectores de 512 bytes desde los primeros discos duros, a partir de la
aprobación del /Formato Avanzado/ en 2010 que incrementa los sectores
a 4096 bytes, presenta otra abstracción más: Un disco con sectores de
4096 bytes que es empleado por el sistema operativo como si fuera de
512[fn:: Al día de hoy, los principales sistemas operativos pueden ya
hacer referencia al nuevo tamaño de bloque, pero la cantidad de
equipos que corren sistemas /heredados/ o de controladores que no
permiten este nuevo modo de acceso limitan una adopción al 100%] tiene
que efectuar, dentro de la lógica de su controlador, una emulación — Y
una modificación de un sólo sector se vuelve un /ciclo
lectura-modificación-escritura/ (/RMW/), que redunda en una espera de
por lo menos una revolución adicional (8ms con un disco de 7200RPM)
del disco antes de que la operación pueda completarse.

Resulta claro que, dados estos cambios en la manera en que debe
referirse a los bloques del disco, el sistema operativo no cuenta ya
con la información necesaria para emplear los algoritmos de
planificación de acceso a disco.

** Almacenamiento en estado sólido
# <<FS_FIS_estado_solido>>

Desde hace cerca de una década va creciendo consistentemente el uso de
medios de almacenamiento de /estado sólido/ — Esto es, medios sin
partes móviles. Las características de estos medios de almacenamiento
son muy distintas de las de los discos.

Si bien las estructuras lógicas que emplean hoy en día prácticamente
todos los sistemas de archivos en uso mayoritario están pensadas
siguiendo la lógica de los medios magnéticos rotativos, como se verá
en esta sección, el empleo de estructuras más acordes a las
características del medio físico. Este es indudablemente un área bajo
intensa investigación y desarrollo, y que seguramente ofrecerá
importantes novedades en los próximos años.

Lo primero que llama la atención de estos medios de almacenamiento es
que, a pesar de ser fundamentalmente distintos a los discos
magnéticos, se presentan ante el sistema operativo como si fueran lo
mismo: En lo que podría entenderse como un esfuerzo para ser
utilizados pronto y sin tener que esperar a que los desarrolladores de
sistemas operativos adecuaran los controladores, se conectan a través
de la misma interfaz y empleando la misma semántica que un disco
rotativo.[fn:: Las unidades de estado sólido cuentan con una /capa de
traducción/ que emula el comportamiento de un disco duro, y presenta
la misma interfaz tanto de bus como semántica.] Esto no sólo evita que
se aprovechen sus características únicas, adoptando restricciones y
criterios de diseño que ahora resultan indudablemente artificiales,
sino que incluso se exponen a mayor stress por no emplearse de la
forma que les resultaría natural.

Antes de ver por qué, conviene hacer un breve repaso de los tipos de
discos de estado solido que hay.  Al hablar de la tecnología sobre la
cual se implementa este tipo de almacenamiento, los principales medios
son:

- NVRAM :: Unidades /RAM No Volátil/. Almacenan la información en
           chips de RAM estándar, con un respaldo de batería para
           mantener la información cuando se desconecta la corriente
           externa. Las primeras unidades de estado sólido eran de
           este estilo; hoy en día son poco comunes en el mercado,
           pero siguen existiendo.

	   Su principal ventaja es la velocidad y durabilidad: El
           tiempo de acceso o escritura de datos es el mismo que el
           que podría esperarse de la memoria principal del sistema, y
           al no haber demoras mecánicas, este tiempo es el mismo
           independientemente de la dirección que se solicite.

	   Su principal desventaja es el precio: En líneas generales,
           la memoria RAM es, por volumen de almacenamiento, cientos
           de veces más cara que el medio magnético. Y si bien el
           medio no se degrada con el uso, la batería sí, lo que
           podría poner en peligro a la supervivencia de la
           información.

	   Estas unidades típicamente se instalan internamente como
           una tarjeta de expansión.

	   #+caption: Unidad de estado sólido basado en RAM: DDRdrive X1 ([[https://en.wikipedia.org/wiki/Solid-state\_drive][Imagen: Wikipedia]])
	   #+attr_latex: width=0.5\textwidth
	   #+attr_html: height="200" width="300"
	   [[./img/estado_solido_ddr_drivex1.jpg]]

- Memoria /flash/ :: Derivada de los /EEPROM/ (/Electrically Erasable
     Programmable Read-Only Memory/, /Memoria de Sólo Lectura
     Programable y Borrable Eléctricamente/). Los EEPROM tienen la
     característica de que, además de lectura y escritura, hay un
     tercer tipo de operación que deben implementar: El /borrado/. Un
     EEPROM ya utilizado debe borrarse antes de volverse a escribir a
     él. La principal característica que distingue a las memorias
     /flash/ de los EEPROMs tradicionales es que el espacio de
     almacenamiento está dividido en muchas /celdas/, y el controlador
     puede leer, borrar o escribir a cada uno de ellos por
     separado.[fn:: Estos dispositivos se conocen como /flash/ en
     referencia a los chips EPROM (antes de que fuera posible borrar
     /eléctricamente/): Estos chips tenían una ventana en la parte
     superior, y debían operar siempre cubiertos con una
     etiqueta. Para borrar sus contenidos, se retiraba la etiqueta y
     se les administraba una descarga lumínica — Un /flash/.]

     El uso de dispositivos /flash/ para almacenamiento de información
     inició hacia 1995 como respuesta a las necesidades de las
     industrias aeroespacial y militar, dada la frecuencia de los
     daños a la información que presentaban los medios magnéticos por
     la vibración. Hoy en día hay dispositivos /flash/ de muy bajo
     costo y capacidad, aunque presentan una gran variabilidad tanto
     en su tiempo de acceso como en su durabilidad. En este sentido,
     existen dos tipos principales de dispositivos /flash/:

  - Almacenamiento primario (SSD) :: Las llamadas formalmente
       /unidad de estado sólido/ (/Solid State Drive/)[fn:: Un
       error muy común es confundir la /D/ con /Disk/, que
       denotaría que llevan un /disco/, un /medio rotativo/] son
       unidades Flash de alta velocidad y capacidad, y típicamente
       presentan una interfaz similar a la que tienen los discos
       duros; hoy en día, la más común es SATA.

       #+caption: Unidad de estado sólido basado en Flash con interfaz SATA ([[https://en.wikipedia.org/wiki/Solid-state\_drive][Imagen: Wikipedia]])
       #+attr_html: height="262" width="400"
       #+attr_latex: width=0.5\textwidth
       [[./img/estado_solido_sata.jpg]]

       Su velocidad de lectura es muy superior y su velocidad de
       escritura (incluyendo el borrado) es comparable a la de los
       discos magnéticos. Su precio por el mismo volumen de
       almacenamento es entre 5 y 10 veces el de los discos
       magnéticos.

       Estas unidades se emplean tanto como unidades
       independientes en servidores, equipos de alto desempeño e
       incluso algunas subportátiles (/netbooks/) o como un
       componente de la tarjeta madre en dispositivos móviles como
       teléfonos y tabletas.

  - Transporte de archivos :: Esta tecnología también está presente en
       las diversas unidades extraíbles o móviles, como las unidades
       USB, SD, Memory Stick, Compact Flash, etc. La principal
       diferencia entre estas son los diferentes conectores que
       emplean; todas estas tecnologías presentan dispositivos que
       varían fuertemente en capacidad, velocidad y durabilidad.

       #+caption: Unidad de estado sólido basado en Flash con interfaz USB ([[https://en.wikipedia.org/wiki/Solid-state\_drive][Imagen: Wikipedia]])
       #+attr_html: height="300" width="335"
       #+attr_latex: width=0.5\textwidth
       [[./img/estado_solido_usb.jpg]]


Independientemente del tipo, las unidades de estado sólido presentan
ventajas ante los discos rotativos, como un muy bajo consumo
eléctrico, operación completamente silenciosa, y resistencia a la
vibración o a los golpes. Además, el medio es /verdaderamente/ de
acceso aleatorio: Al no ser ya un disco, desaparecen tanto la demora
de movimiento de cabezas como la rotacional.

*** Desgaste del medio

La memoria Flash presenta patrones de desgaste muy distintos de los que
presentan otros medios. La memoria Flash tiene capacidad de
aguantar un cierto número de operaciones de borrado por página[fn::
Dependiendo de la calidad, va entre las 3,000 y 100,000] antes de
comenzar a degradarse y fallar. Las
estructuras tradicionales de sistemas de archivos basados en disco
/concentran/ una gran cantidad de modificaciones frecuentes a lo largo
de la operación normal del sistema en ciertas regiones
clave: Las tablas de asignación y directorios registran muchos más
cambios que la región de datos.

Casi todos los controladores de discos Flash cuentan con mecanismos de
/nivelamiento de escrituras/ (/write leveling/). Este mecanismo busca
reducir el desgaste focalizado modificando el mapeo de los sectores
que ve el sistema operativo respecto a los que son grabados /en
verdad/ en el medio: En vez de actualizar un bloque (por ejemplo, un
directorio) /en su lugar/, el controlador le asigna un nuevo bloque de
forma transparente, y marca el bloque original como libre.

Los mecanismos más simples de nivelamiento de escrituras lo hacen
únicamente intercambiando los bloques libres con los recién
reescritos; mecanismos más avanzados buscan nivelar el nivel de
reescritura en toda la unidad reubicando periódicamente también a los
bloques que no son modificados, para no favorecerlos injustamente y
hacer un mejor balanceo de uso.

*** Emulación de discos

Hoy en día, casi la totalidad de medios de estado sóldo se presentan
ante el sistema con una interfaz que emula la de los discos, la /FTL/
(/Flash Translation Layer/, /Capa de Traducción de Flash/). La ventaja
de esta emulación es que no hizo falta desarrollar controladores
adicionales para comenzar a emplear estos medios. La desventaja, sin
embargo, es que al ocultarse el funcionamiento /real/ de las unidades
de estado sólido, el sistema operativo no puede aprovechar las
ventajas estructurales — Y más importante aún, no puede evitar las
debilidades inherentes al medio.

Uno de los ejemplos más claros de esta falta de control real del medio
la ilustra el [[https://lwn.net/Articles/353411/][artículo de Valerie Aurora (2009)]], que menciona que
tanto la poca información públicamente disponible acerca del
funcionamiento de los controladores como los patrones de velocidad y
desgaste de los mismos apuntan a que la estructura subyacente de casi
todos los medios de estado sólido es la de un /sistema de archivos
estructurado en bitácora/. Aurora indica que hay varias operaciones
que no pueden ser traducidas eficientemente a través de esta capa de
emulación, y que seguramente permitirían un mucho mejor
aprovechamiento del medio. Como se mencionó en la sección
\ref{FS_log_structured} (/Sistemas de archivo estructurados en
bitácora/), si bien varios de estos sistemas de archivos han
presentado implementaciones completamente utilizables, la falta de
interés ha llevado a que muchos de estos proyectos sean abandonados.

En su [[http://lwn.net/Articles/528617/][artículo de 2012]], Neil Brown apunta a que Linux tiene una
interfaz apta para hablar directamente con dispositivos de estado
sólido, llamada =mtd= — /memory technology devices/, /dispositivos de
tecnología de memoria/.

Si bien los discos duros se han empleado por ya 50 años y los sistemas
de archivos están claramente desarrollados para aprovechar sus
detalles físicos y lógicos, el uso de los dispositivos de estado
sólido apenas está despegando en la última década. Y si bien esta
primer aproximación que permite emplear esta tecnología
transparentemente es /suficientemente buena/ para muchos de los usos
básicos, sin duda hay espacio para mejorar. Este es un tema que
seguramente brinda amplio espacio para investigación y desarrollo para
los próximos años.

* RAID: Más allá de los límites físicos
# <<FS_FIS_RAID>>
En la sección \ref{FS_conceptos} se presentó muy escuetamente al
concepto de /volumen/, mencionando que un volumen /típicamente/
coincide con una partición, aunque no siempre es el caso — Sin
profundizar más al respecto. En esta sección se presentará uno de los
mecanismos que permite combinar diferentes /dispositivos físicos/ en
un sólo volumen, llevando –bajo sus diferentes modalidades– a mayor
confiabilidad, rendimiento y espacio disponible.

El esquema más difundido para este fin es conocido como /RAID/,
/Arreglo Redundante de Discos Baratos/ (/Redundant Array of
Inexpensive Disks/)[fn:: Ocasionalmente se presenta a RAID como
acrónimo de /Arreglo Redundante de Discos Independientes/ (/Redundant
Array of Independent Disks/)], propuesto en 1988 por David Patterson,
Garth Gibson y Randy Katz ante el diferencial que se presentaba (y se
sigue presentando) entre el avance en velocidad y confiabilidad de las
diversas áreas del cómputo en relación al almacenamiento magnético.

Bajo los esquemas RAID queda sobreentendido que los diferentes discos
que forman parte de un volumen son del mismo tamaño. Si se remplaza un
disco de un arreglo por uno más grande, la capacidad /en exceso/ que
tenga éste sobre los demás discos será desperdiciada.

Por muchos años, para emplear un /arreglos RAID/ era necesario contar
con controladores dedicados, que presentaban al conjunto como un
dispositivo único al sistema operativo. Hoy en día, prácticamente
todos los sistemas operativos incluyen la capacidad de integrar varias
unidades independientes en un arreglo por software; esto conlleva un
impacto en rendimiento, aunque muy pequeño. Hay también varias
tecnologías presentes en distintos sistemas operativos modernos que
heredan las ideas presentadas por RAID, pero integrándolos con
funciones formalmente implementadas por capas superiores.

RAID no es un sólo esquema, sino que especifica un /conjunto/ de
/niveles/, cada uno de ellos diseñado para mejorar distintos aspectos
del almacenamiento en discos. Se exponen a continuación las
características de los principales niveles en uso hoy en día.

** RAID nivel 0: División en /franjas/

El primer nivel de RAID brinda una ganancia tanto en espacio total,
dado que presenta a un volumen grande en vez de varios discos más
pequeños (simplificando la tarea del administrador) como de velocidad,
dado que las lecturas y escrituras al volumen ya no estarán sujetas al
movimiento de una sola cabeza, sino que habrá una cabeza independiente
por cada uno de los discos que conformen al volumen.

#+attr_latex: width=0.7\textwidth
#+label: FS_FIS_raid_0
#+caption: Cinco discos organizados en RAID 0
[[./img/dot/raid_0.png]]

Los discos que participan en un volumen RAID 0 no están sencillamente
/concatenados/, sino que los datos son /divididos en franjas/ (en
inglés, el proceso se conoce como /striping/, de la palabra /stripe/,
franja; algunas traducciones al español se refieren a este proceso
como /bandeado/). Esto hace que la carga se reparta de forma
uniforme entre todos los discos, y asegura que todas las
transferencias mayores al tamaño de una franja provengan de más de un
disco independiente.

#+attr_latex: width=0.6\textwidth
#+label: FS_FIS_franjas_raid_0
#+caption: División de datos en /franjas/
[[./img/ditaa/franjas_raid_0.png]]

La confiabilidad del volumen, sin embargo, disminuye
respecto a si cada uno de los discos se manejara por separado: Basta
con que uno de los discos presente daños para que la información
contenida en el volumen se pierda.

Un arreglo RAID nivel 0 puede construirse con un mínimo de dos discos.

** RAID nivel 1: Espejo

Este nivel está principalmente orientado a aumentar la confiabilidad
de la información: Los datos son grabados de forma /simultánea e
idéntica/ en todos los discos que formen parte del volumen. El costo
de mantener los datos en espejo, claro está, es el del espacio
empleado: En su configuración habitual, de dos discos por volumen, el
50% del espacio de almacenamiento se pierde por fungir como respaldo
del otro 50%.

La velocidad de acceso a los datos bajo RAID 1 es mayor a la que se
lograría con un disco tradicional: Basta con obtener los datos de uno
de los discos; el controlador RAID (sea el sistema operativo o una
implementación en hardware) puede incluso programar las solicitudes de
lectura para que se vayan repartiendo entre ambas unidades. La
velocidad de escritura se ve levemente reducida, dado que hay que
esperar a que ambos discos escriban la información.

#+label: FS_FIS_raid_1
#+caption: Dos discos en espejo con RAID 1
#+attr_latex: width=0.3\textwidth
[[./img/dot/raid_1.png]]

Un arreglo RAID nivel 1 se construye típicamente con dos discos.

** Los niveles 2, 3 y 4 de RAID

Los siguientes tres niveles de RAID combinan propiedades de los
primeros junto con un algoritmo de verificación de integridad y
corrección de errores. Estos han caído casi por completo en el desuso
dado que los otros niveles, y muy en particular el nivel 5, ofrecen
las mismas características, pero con mayor confiabilidad

** RAID nivel 5: Paridad dividida por bloques

El nivel 5 de RAID proporciona un muy buen equilibrio respecto a las
características que se han mencionando:  brinda el espacio total
de almacenamiento de todos los discos que formen parte del volumen
/menos uno/. Para cada una de las /franjas/, RAID5 calcula un bloque
de /paridad/.

Para obtener una mayor tolerancia a fallos, este bloque de paridad no siempre va al mismo
disco, sino que se va repartiendo entre todos los discos del volumen,
/desplazándose/ a cada franja, de modo que /cualquiera de los discos
puede fallar/, y el arreglo continuará operando sin pérdida de
información. Esta debe notificarse al administrador del sistema,
quien reemplazará al disco dañado lo antes posible (dado que, de no
hacerlo, la falla en un segundo disco resultará en la pérdida de toda
la información).

En equipos RAID profesionales es común contar con discos de reserva
/en caliente/ (/hot spares/): Discos que se mantienen apagados pero
listos para trabajar. Si el controlador detecta un disco dañado, sin
esperar a la intervención del administrador, desactiva al disco
afectado y activa al /hot spare/, reconstruyendo de inmediato la
información a partir de los datos en los discos /sanos/.

#+attr_latex: width=0.7\textwidth
#+label: FS_FIS_franjas_raid_5
#+caption: División de datos en /franjas/, con paridad, para RAID 5
[[./img/ditaa/franjas_raid_5.png]]

Dependiendo de la configuración, la velocidad de acceso de este nivel
puede ser es ligeramente menor que la obtenida de los discos
sin RAID, o ligeramente menor a la que se logra con RAID
nivel 0. Dado que la electrónica en los discos actuales
notificará explícitamente al sistema operativo en caso de fallo de
lectura, cuando el sistema requiere leer datos, estos pueden ser
solicitados únicamente a $n-1$ discos (e ignorar al de paridad); si el arreglo RAID está
configurado para verificar la paridad en lecturas, todas
las lecturas tendrán que obtener la franja correspondiente de todos
los discos del arreglo para poder calcularla.

RAID 5 opera con un algoritmo de verificación y recuperación
sorprendentemente eficiente y simple: El de una /suma XOR/, ilustrado
en la figura \ref{FS_FIS_redundancia_xor}. La operación booleana XOR
(de /Exclusive OR/) suma los bits individuales, columna por
columna. Si es un número par, almacena un 0, si es impar, almacena
un 1. Esta operación es muy eficiente computacionalmente.

#+attr_latex: width=0.5\textwidth
#+label: FS_FIS_redundancia_xor
#+caption: Para cada franja, el disco de paridad guarda la suma XOR de los bits de las franjas correspondientes de los otros discos; no importa cuál disco falle, sus datos pueden recuperarse haciendo un XOR de los datos de los demás.
[[./img/ditaa/redundancia_xor.png]]

Las escrituras son invariablemente más lentas respecto tanto ante
la ausencia de RAID como en niveles 0 y 1, dado que siempre
tendrá que recalcularse la paridad; en el caso de una escritura
mínima (menor a una franja) tendrá que leerse la franja entera de
todos los discos participantes en el arreglo, recalcularse la
paridad, y grabarse en el disco correspondiente.

Cuando uno de los discos falla, el arreglo comienza a trabajar en el
/modo interino de recuperación de datos/ (/Interim data recovery
mode/, también conocido como /modo degradado/), en el que todas las lecturas involucran a todos los discos, ya
que tienen que estar recalculando y /rellenando/ la información que
provendría del disco dañado.

#+attr_latex: width=0.7\textwidth
#+label: FS_FIS_raid_5
#+caption: Cinco discos organizados en RAID 5
[[./img/dot/raid_5.png]]

Para implementar RAID nivel 5 son necesarios por lo menos 3 discos,
aunque es común verlos más /anchos/, pues de este modo se desperdicia
menos espacio en paridad. Si bien teóricamente un arreglo nivel 5
puede ser arbitrariamente ancho, en la práctica es muy raro ver
arreglos con más de 5 discos: Tener un arreglo más ancho aumentaría la
probabilidad de falla. Si un arreglo que está ya operando en el modo
interino de recuperación de datos se encuentra con una falla en
cualquiera de sus discos, tendrá que reportar un fallo irrecuperable.

** RAID nivel 6: Paridad por redundancia P+Q

Se trata nuevamente de un nivel de RAID muy poco utilizado. Se basa en
el mismo principio que el de RAID 5 pero, empleando dos distintos
algoritmos para calcular la paridad, permite la pérdida de hasta dos
de los discos del arreglo. La complejidad computacional es
sensiblemente mayor a la de RAID 5, no sólo porque se trata de un
segundo cálculo de paridad, sino porque este cálculo debe hacerse
empleando un algoritmo distinto y más robusto — Si bien para obtener
la paridad $P$ basta con hacer una operación /XOR/ sobre todos los
segmentos de una /franja/, la segunda paridad $Q$ típicamente emplea
al /algoritmo Reed-Solomon/, /paridad diagonal/ o /paridad dual
ortogonal/. Esto conlleva a una mayor carga al sistema, en caso de que
sea RAID por software, o a que el controlador sea de mayor costo por
implementar mayor complejidad, en caso de ser hardware dedicado.

#+attr_latex: width=0.7\textwidth
#+label: FS_FIS_raid_6
#+caption: Cinco discos organizados en RAID 6
[[./img/dot/raid_6.png]]

El nivel 6 de RAID puede implementarse con 4 o más unidades, y si
bien el espacio dedicado a la redundancia se incrementa a dos discos,
la redundancia adicional que ofrece este esquema permite crear
volúmenes con un mayor número de discos.

** Niveles combinados de RAID

Viendo desde el punto de vista de la abstracción presentada, RAID
toma una serie de dispositivos de bloques y los /combina/ en otro
dispositivo de bloques. Esto significa que puede tomarse una serie de
volúmenes RAID y combinarlos en uno solo, aprovechando las
características de los diferentes niveles.

Si bien pueden combinarse arreglos de todo tipo, hay combinaciones más
frecuentes que otras. Con mucho, la más popular es la de los niveles
1 + 0 — Esta combinación, frecuentemente llamada sencillamente /RAID
10/, ofrece un máximo de redundancia y rendimiento, sin sacrificar
demasiado espacio.

#+attr_latex: width=0.7\textwidth
#+label: FS_FIS_raid_10
#+caption: Seis discos organizados en RAID 1+0
[[./img/dot/raid_10.png]]

Con RAID nivel 10 se crean volúmenes que suman por franjas unidades en
espejo (un volumen RAID 0 compuesto de varios volúmenes RAID 1). En
caso de fallar cualquiera de las unidades del arreglo, ésta puede ser
reemplazada fácilmente, y su reemplazo no significará un trabajo tan
intensivo para el arreglo entero (sólo para su disco espejo).

Bajo este esquema, en el peor de los casos, un volumen con $n$
discos físicos está conformado por $n \over 2$ volúmenes nivel 1, y por tanto
puede soportar la pérdida de hasta $n \over 2$ discos — Siempre
que estos no formen parte de un mismo volumen nivel 1.

Esta combinación ilustra cómo el órden de los
factores /sí altera/ al producto: Si en vez de la concatenación de
varias unidades espejeadas (un volumen nivel 0 compuesto de varios
volúmenes nivel 1) se armara el arreglo en órden inverso (esto
es, como el espejeo de varias unidades concatenadas por franjas),
ante un primer análisis parecería se obtienen los mismos beneficios —
Pero analizando lo que ocurre en caso de falla, resulta claro que el
nivel de redundancia resulta mucho menor.

#+attr_latex: width=0.65\textwidth
#+label: FS_FIS_raid_01
#+caption: Seis discos organizados en RAID 0+1
[[./img/dot/raid_01.png]]

En este caso, el arreglo soportará también el fallo de hasta $n \over
2$ de sus discos, pero únicamente si ocurren /en el mismo volumen RAID
1/ del espejo.

Dado que RAID opera meramente agregando /dispositivos de bloques/ en
un nuevo dispositivo del mismo tipo, no tiene conocimiento de la
información subyacente. Por tanto, si se perdieran al mismo tiempo el
disco 1 (del subvolumen 1) y el disco 5 (del subvolumen 2), resultaría
en pérdida de datos.[fn:: O por lo menos, en una tarea de
reconstrucción manual, dada que la información completa existe. Sin
embargo, ambos volúmenes RAID 1 estarían dañados e incompletos.]

* Manejo avanzado de volúmenes
# <<FS_FIS_man_av_vol>>

Los esquemas RAID vienen, sin embargo, de fines de la década de 1980,
y si bien han cambiado el panorama del almacenamiento, en los más de
20 años desde su aparición, han sido ya superados. El énfasis en el
estudio de RAID (y no tanto en los desarrollos posteriores) se
justifica dada la limpieza conceptual que presentan, y dado que
esquemas posteriores incluso hacen referencia explícita al nivel de
RAID que /estarían reemplazando/ en su documentación.

A continuación se presentan brevemente dos esquemas avanzados de
gestión de volúmenes, principalmente ilustrando la dirección en que
parece ir avanzando la industria en este campo. Dado que no presentan
nuevos conceptos sino que sólo ilustran cómo se integran los que se
han expuesto en las últimas páginas, la exposición se limitará a
presentar ejemplos de aplicación, sin entrar más que a un nivel
descriptivo de su funcionamiento.

** LVM: el Gestor de Volúmenes Lógicos

Una evolución natural de los conceptos de RAID es el /LVM2/ (segunda
generación del /Logical Volume Manager/, o /Gestor de Volúmenes
Lógicos/) de Linux. La lógica de operación de LVM está basada en los
siguientes conceptos:

- Volumen físico :: Cada uno de los discos o unidades disponibles
- Grupo de volúmenes :: Conjunto de volúmenes físicos que serán
     administrados como una sola entidad
- Volumen lógico :: Espacio dentro del grupo de volúmenes que se presenta
		    como un dispositivo, y que puede alojar sistemas
                    de archivos.

El esquema es limpio y elegante: LVM es una interfaz que permite, como
dos pasos independientes, agregar diferentes /volúmenes físicos/ a un
/grupo de volúmenes/, para posteriormente –y siguiendo las necesidades
del administrador del sistema, ya independientes del tamaño de las
unidades físicamente existentes– crear las /unidades lógicas/, donde
se alojarán los sistemas de archivos propiamente.

Este esquema permite naturalmente una funcionalidad comparable con
RAID 0: Puede crearse un grupo de volúmenes con todos los discos que
disponibles, y dentro de este crear un volumen lógico
único. Dependiendo de la configuración, este volumen lógico puede
crecer abarcando todos los discos en cuestión, sea como simple
concatenación o dividiéndose en franjas.

Permite también la creación de unidades /espejo/, con una operación
a grandes rasgos equivalente a la de RAID1. Incluso, dentro de un
mismo grupo de volúmenes, pueden existir tanto volúmenes lógicos
espejeados como otros que no lo estén, a diferencia de la estricta
rigidez de RAID.

Para los niveles 4, 5 y 6 de RAID, la correspondencia es más directa
aún: Al crear un volumen, se le puede solicitar a LVM al crear un
volumen lógico que cree un volumen con ese nivel de RAID — Obviamente,
siempre que cuente con suficientes volúmenes físicos.

El esquema de LVM no brinda, pues, funcionalidad estrictamente
distinta a la que presenta RAID — Pero da al administrador del sistema
flexibilidad: ampliar o reducir el espacio dedicado a cada uno de los
volúmenes, incluso en un sistema en producción y con datos.

LVM ofrece varias funcionalidades adicionales, como las /fotografías/
(/snapshots/) o varios esquemas de reemplazo de disco; si bien hay
mucho más que podría decirse de LVM, no se profundiza más en esta
herramienta dado que excede del objetivo del presente material.

** ZFS
# <<FS_FIS_zfs>>

Si bien LVM realiza una importante tarea de simplificación en la
administración del sistema, su operación sigue siendo orientada a
/bloques/: Los volúmenes lógicos deben aún ser formateados bajo el
sistema de archivos que el administrador del sistema considere acorde
para la tarea requerida.

ZFS[fn:: El nombre /ZFS/ proviene de /Zettabyte File System/. Los
diseñadores de ZFS indican, sin embargo, que esto no es para indicar
que ZFS sea capaz de direccionar hasta zettabytes de información, sino
que será /el último sistema de archivos/ que cualquier administrador
requerirá.] fue desarrollado por Sun Microsystems desde el año 2001,
forma parte del sistema operativo /Solaris/ desde el 2005, y hoy en
día puede emplearse desde los principales sistemas operativos
libres.[fn:: ZFS no puede ser incorporado íntegramente al núcleo de
Linux por /incompatibilidad de licencias/: Si bien ambos son software
libre, los modelos de licenciamiento GPL (de Linux) y CDDL (de ZFS)
son incompatibles.] Y si bien ZFS resulta suficientemente atractivo
tan sólo por haber sido diseñado para que el usuario nunca más se tope
con un límite impuesto por el sistema operativo, el principal cambio
que presenta al usuario es una forma completamente distinta de
referirse al almacenamiento.

En primer término, al igual que LVM presenta una primer integración
entre conceptos, permitiendo unir de diferentes maneras varios
dispositivos físicos en un dispositivo lógico, ZFS incluye en la misma
lógica administrativa al sistema de archivos: En la configuración
estándar, basta conectar una unidad al sistema para que ésta aparezca
como espacio adicional disponible para los usuarios. El espacio
combinado de todas las unidades conforma un /fondo de almacenamiento/
(/storage pool/).

La lógica de ZFS parte de que operará una /colección/ de sistemas de
archivos en una organización jerárquica. Pero a diferencia del esquema
tradicional Unix en que cada sistema de archivos es preparado desde un
principio para su función, en ZFS se pueden aplicar límites a
jerarquías completas. Bajo un esquema ZFS, la creación y el montaje de
un sistema de archivos es una operación sencilla — Al grado que se
presenta como recomendación que, para cada usuario en el sistema, se
genere un sistema de archivos nuevo e independiente.

Una de las principales diferencias con los sistemas de archivos
tradicionales es el manejo del espacio vacío: El espacio disponible
total del fondo de almacenamiento se reporta como disponible /para
todos los sistemas de archivos/ que formen parte de éste. Sin embargo,
se pueden indicar /reservas/ (mantener un mínimo del espacio
especificado disponible para determinado subconjunto de sistemas de
archivos dentro de la colección) y /límites/ (evitar que el uso de una
colección exceda el almacenamiento indicado) para las necesidades de
las diferentes regiones del sistema.

* Otros recursos

- [[http://constantin.glez.de/blog/2010/03/opensolaris-zfs-deduplication-everything-you-need-know][OpenSolaris ZFS Deduplication: Everything You Need to Know]]
  (Constantin Gonzalez, 2010)

- [[http://www.freebsd.org/doc/en_US.ISO8859-1/books/handbook/filesystems-zfs.html][The Z File System (ZFS)]] (FreeBSD Handbook)

- [[http://www.cs.berkeley.edu/~brewer/cs262/LFS.pdf][The Design and Implementation of a Log-Structured File System]]
  (Mendel Rosenblum, J. K. Ousterhout, 1992)

- [[http://lwn.net/Articles/529077/][A hash-based DoS attack on Btrfs]] (LWN)

- [[http://linux.slashdot.org/story/12/12/15/0055217/denial-of-service-attack-found-in-btrfs-file-system][Denial-of-Service Attack Found In Btrfs File-System]] (Slashdot)

- [[http://storage.toshiba.com/docs/services-support-documents/toshiba_4kwhitepaper.pdf][4K Sector Disk Drives: Transitioning to the Future with Advanced
  Format Technologies]] (Michael E. Fitzpatrick, Toshiba, 2011)

- [[http://delivery.acm.org/10.1145/150000/146943/p26-rosenblum.pdf][The Design and Implementation of a Log-Structured File System]] (John
  K. Ousterhout, Mendel Rosenblum, 1992)

- [[http://www.informatik.uni-osnabrueck.de/papers_pdf/2005_07.pdf][LogFS — Finally a scalable flash file system]] (Jörn Engel, Robert
  Mertens, 2005)

- [[https://lwn.net/Articles/353411/][Log-structured file systems: There's one in every SSD]] (Valerie
  Aurora, 2009)

- [[http://lwn.net/Articles/528617/][JFFS2, UBIFS, and the growth of flash storage]] (Neil Brown, 2012)

- [[http://www.cs.cmu.edu/%7Egarth/RAIDpaper/Patterson88.pdf][A case for Redundant Arrays of Inexpensive Disks]] (Patterson,
  Gibson, Katz 1988)

- [[http://insecurityit.blogspot.mx/2013/06/unidades-de-estado-solido-el-reto-de-la.html][Unidades de estado sólido. El reto de la computación forense en el mundo de los semiconductores]] (Cano Martinez, 2013)

- [[http://dx.doi.org/10.1038/ncomms2990][Non-volatile memory based on the ferroelectric photovoltaic effect]]
  (Guo, You, Zhow et. al., 2013)

- [[http://elinux.org/images/b/b6/EMMC-SSD_File_System_Tuning_Methodology_v1.0.pdf][eMMC/SSD File System Tuning Methodology]], Cogent Embedded, 2013
  (referido desde [[http://lwn.net/Articles/557220/][A flash filesystem tuning guide]], LWN, julio 2013)
