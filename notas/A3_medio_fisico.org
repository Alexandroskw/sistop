#+SETUPFILE: ../setup_notas.org
#+TITLE: El medio físico y el almacenamiento

* El medio físico
# <<FS_FIS>>

Hasta este punto, y siguiendo las prácticas a que la realidad de los
últimos 40 años nos han acostumbrado, hemos empleado el término
genérico de /disco/ para nuestra discusión.

En este apéndice se abordan en primer término las características
principales del medio aún prevalente, los discos duros magnéticos
rotativos, y se presenta una introducción a las diferencias que
encontraremos en otros medios, como los discos ópticos y los de
estado sólido, y las implicaciones que éstos tienen sobre el material
presentado en el capítulo \ref{FS}.

Cabe mencionar que la razón de separar este contenido hacia un
apéndice es que, si bien estas funciones resultan relevantes para los
sistemas operativos y estos cada vez más van asumiendo las funciones
que aquí serán descritas, estas comenzaron siendo implementadas por
hardware especializado; es apenas hasta la aparición de los esquemas
de manejo avanzado de volúmenes, que serán cubiertos en la sección
\ref{FS_FIS_man_av_vol}, que caen nuevamente dentro del ámbito
exclusivo del sistema operativo.

** Discos magnéticos rotativos

El principal medio de almacenamiento que hemos empleado en los
últimos 40 años es el /disco magnético/. Hay dos tipos diferentes de
disco, aunque la lógica de su funcionamiento es la misma: Los /discos
duros/ y los /discos flexibles/ (o /floppies/).

La principal diferencia entre estos es que los primeros son
típicamente almacenamiento /interno/ en los equipos de cómputo, y los
segundos están pensados para ser almacenamiento /transportable/. Los
discos duros tienen mucha mayor capacidad y son mucho más rápidos,
pero a cambio de ello, mucho más sensibles a la contaminación por
partículas de polvo y a daños mecánicos.

Un disco flexible es una hoja de material plástico, muy similar al
empleado en las cintas magnéticas, resguardado por un estuche
plástico. Al insertarse el disco en la unidad lectora, esta lo hace
girar sujetándolo por el centro, y las cabezas lectoras[fn:: en un
principio una sola; posteriormente aparecieron las unidades de doble
cara, con dos cabezas lectoras] se deslizan por una ventana que tiene
el estuche. La mayor parte de los discos flexibles presentaban
velocidades de rotación de entre 300 y 400 revoluciones por minuto.

A lo largo de más de 20 años se presentaron muy diferentes formatos
físicos siguiendo esta misma lógica, designándose principalmente por
su tamaño (en pulgadas). La capacidad de los discos, claro está, fue
creciendo con el paso de los años — Esto explica la aparente
contradicción de que los discos más chicos físicamente tenían más
capacidad que los más grandes.

#+caption: Principales formatos de disco flexible que se popularizaron en el mercado
|-----------------------+-------------+---------------+--------------|
|                       |  8 pulgadas | 5.25 pulgadas | 3.5 pulgadas |
|-----------------------+-------------+---------------+--------------|
| Fecha de introducción |        1971 |          1976 |         1982 |
| Capacidad             | 150KB-1.2MB |   110KB-1.2MB | 264KB-2.88MB |
| Velocidad (kbit/s)    |          33 |       125-500 |     250-1000 |
| Pistas por pulgada    |          48 |         48-96 |          135 |
|-----------------------+-------------+---------------+--------------|

El nombre de /disco duro/ o /disco flexible/ se debe al medio empleado
para el almacenamiento de la información: Mientras que los discos
flexibles emplean, como mencionamos, una hoja plástica flexible, los
discos duros son metálicos. Los discos están /permanentemente/
montados sobre un eje, lo que permite que tengan una velocidad de giro
entre 20 y 50 veces más rápida que los discos flexibles — Entre 4,200 y
15,000 revoluciones por minuto (RPM), esto es, con una demora
rotacional de entre 2 y 7.14 milisegundos..

Además, a excepción de algunos modelos tempranos, los discos duros
constituyen un paquete cerrado y sellado que incluye las cabezas de
lectura y escritura, y toda la electrónica de control. Esto permite
que los discos duros tengan densidades de almacenamiento y velocidades
de transmisión muy superiores a la de los discos flexibles: Los
primeros discos duros que se comercializaron para computadoras
personales eran de 10MB (aproximadamente 70 discos flexibles de su
época), y actualmente hay ya discos de 4TB. La velocidad máxima de
transferencia sostenida hoy en día es superior a los 100MB por
segundo, 100 veces más rápido que la última generación de discos
flexibles.

Para medir la eficiencia de un disco duro, el otro dato importante es
el tiempo que toma la cabeza en moverse a través de la superficie del
disco. Hoy en día, las velocidades más comunes son de 20ms para un
/recorrido completo/ (desde el primer hasta el último sector), y entre
0.2ms y 0.8ms para ir de un cilindro al inmediato siguiente. Como
punto de comparación, el recorrido completo en una unidad de disco
flexible toma aproximadamente 100ms, y el tiempo de un cilindro al
siguiente va entre 3 y 8ms.

*** Notación C-H-S

Independientemente de la tecnología, la manera en que el sistema
operativo hace referencia a la ubicación de un bloque de información
en el disco es conocido como la /notación C-H-S/ — Indicando el
cilindro, cabeza y sector (/Cylinder, Head, Sector/) de la
información. Esto permite mapear el espacio de almacenamiento de un
disco a un espacio tridimensional, con cual resulta trivial ubicar a
alguna estructura en una región contigua.

#+attr_html: height="451" width="625"
#+attr_latex: width=0.8\textwidth
#+label: FS_FIS_disco_duro
#+caption: Coordenadas de un disco duro, presentando cada uno de sus sectores en C-H-S (Silberschatz, p.458)
[[./img/disco_duro.png]]

La /cabeza/ indica a cuál de las superficies del disco nos referimos;
en un disco flexible, tenemos sólo dos cabezas, pero en un disco duro
es común tener varios /platos/ paralelos. Todas las cabezas van fijas
a un mismo motor, y no pueden moverse de forma independiente.

El /cilindro/ indica la distancia del centro a la orilla del disco.

Un /sector/ es un segmento de arco de uno de los cilindros.

Es frecuente ver referencias a una /pista/ (/track/), esto es, a un
cilindro en una de las superficies del disco — Un archivo almacenado
secuencialmente ocupa /sectores adyacentes/ a lo largo de una misma
pista.

*** Algoritmos de planificación de acceso a disco

Como hemos visto, las transferencias a disco son uno de los procesos
más lentos de los que gestiona el sistema operativo. Cuando éste
tiene varias solicitudes de transferencia pendientes, resulta
importante encontrar un mecanismo óptimo para realizar la
transferencia, minimizando el tiempo de demora. Presentaremos a
grandes rasgos tres de los algoritmos de planificación de acceso a
disco — Pero abordaremos también el por qué estos hoy en día casi no
son empleados.

Como con los demás escenarios que hemos abordado analizando
algoritmos, para analizar su rendimiento tenemos que presentar una
/cadena de referencia/. En este caso, trabajaremos con un disco
hipotético de 200 cilindros, la cadena de solicitudes /98, 183, 37,
122, 14, 124, 65, 67/, y teniendo la cabeza al inicio de la operación
en el cilindro 53.

Veamos, pues, de forma gráfica la respuesta que presentarían los
distintos algoritmos ante la cadena de referencia dada, y procedamos a
comparar a los algoritmos en cuestión.


#+attr_latex: width=0.9\textwidth
#+label: FS_FIS_mov_cabeza_por_algoritmo
#+caption: Movimientos de las cabezas bajo los diferentes algoritmos planificadores de acceso a disco, indicando la distancia total recorrida por la cabeza bajo cada uno, iniciando con la cabeza en la posición 53. Para SCAN, LOOK y C-SCAN, asumimos que iniciamos con la cabeza en dirección decreciente.
[[./img/gnuplot/mov_cabeza_por_algoritmo.png]]

- FIFO :: Al igual que cuando hablamos de algoritmos de asignación de
          procesador y de reemplazo de páginas, el primero y más
          sencillo de implementar es el /FIFO/ — /Primero en llegar,
          primero en servirse/. Este algoritmo puede verse como muy
          /justo/, aunque sea muy poco eficiente: El movimiento total
          de cabezas para el caso planteado es de 640 cilindros,
          equivalente a poco más que recorrer de extremo a extremo el
          disco completo tres veces. Esto es, despreciando la demora
          rotacional y el tiempo que le toma al brazo detenerse por
          completo antes de volver a moverse, esta lectura tomaría un
          mínimo de 60ms, siendo el recorrido completo del disco 20ms.

	  Podemos encontrar al causante de buena parte de esta demora en la
	  quinta posición de la cadena de referencia: Entre solicitudes para el
	  cilindro 122 y 124, tenemos una solicitud al 14, que significa un
	  desplazamiento de $(122-14) + (124-14) = 218$ sectores.

- SSTF :: Ahora bien, si el factor que impone la principal demora es
          el movimiento de la cabeza, el segundo algoritmo busca
          reducir al mínimo el movimiento de la cabeza: /SSTF/
          (/Shortest Seek Time First/, /Tiempo de búsqueda más corto a
          continuación/) es el equivalente en este ámbito del
          /Shortest Job First/ que vimos en planificación de procesos
          — con la ventaja de que no estamos prediciendo
          comportamiento futuro, sino que tenemos ya la lista de
          solicitudes pendientes. Empleando SSTF, el tiempo de
          desplazamiento para este caso se reduce a tan sólo 236
          cilindros — muy cerca del mínimo absoluto posible.

	  Una desventaja de SSTF es que puede llevar a la inanición:
          Si hay una gran densidad de solicitudes para cilindros en
          determinada zona del disco, una solicitud para un cilindro
          alejado puede quedar a la espera
          indefinidamente. Ejemplifiquemo esto con una serie de
          solicitudes distinta a la que estamos tomando como
          referencia: Si el sistema tuviera la cadena de solicitudes
          /15, 175, 13, 20, 14, 32, 40, 5, 6, 7/, SSTF /penalizaría/ a
          la segunda solicitud (175) hasta terminar con los cilindros
          bajos.

- Familia de algoritmos /de elevador/ (SCAN, LOOK, C-SCAN) :: Hablamos
     en este tercer lugar no de un algoritmo, sino que de una
     /familia/, dado que parten de la misma idea, pero con
     modificaciones menores llevan a que el patrón de atención
     resultante sea muy distinto.

     El planteamiento base para el arlgoritmo básico de elevador
     (SCAN) busca evitar la inanición, minimizando al mismo tiempo el
     movimiento de las cabezas. Su lógica marca que la cabeza debe
     recorrer el disco de extremo a extremo, como si fuera un elevador
     en un edificio alto, atendiendo a todas las solicitudes que haya
     pendientes en su camino. Si bien los recorridos para ciertos
     patrones pueden resultar en mayores desplazamientos a los que
     daría SSTF, la garantía de que ningún proceso esperará
     indefinidamente lo hace muy atractivo.

     Atender la cadena de referencia con la que estamos trabajando
     bajo SCAN, asumiendo un estado inicial /descendente/ (esto es, la
     cabeza está en el cilindro 53 y va bajando) da un recorrido total
     de 236 cilindros; empleando LOOK, se reduce a 208 cilindros, y
     evita el movimiento innecesario hasta el límite del disco.

     Una primer (y casi obvia) modificación a este algoritmo sería,
     cada vez que la cabeza se detenga para satisfacer una solicitud,
     verificar si hay alguna otra solicitud pendiente en la /dirección
     actual/, y de no ser así, emprender el camino de regreso sin
     llegar a la orilla del disco. Esta modificación es frecuentemente
     descrita como /LOOK/.

     Sin embargo, el patrón de atención a solicitudes de SCAN y LOOK
     dejan qué desear: Al llegar a un extremo del recorrido, es
     bastante probable que no haya ninguna solicitud pendiente en la
     primer mitad del recorrido de vuelta (dado que acaban de ser
     atendidas). El tiempo que demora atender a una solictud se
     compone de la suma del desplazamiento de la cabeza y la demora
     rotacional (que depende de cuál sector del cilindro nos fue
     solicitado). Para mantener una tasa de transferencia más
     predecible, el algoritmo /C-SCAN/ (SCAN Circular) realiza las
     operaciones en el disco únicamente en un sentido — Si el
     algoritmo lee en órden /descendente/, al llegar a la solicitud
     del cilindro más bajo, saltará de vuelta hasta el más alto para
     volver a iniciar desde ahí. Esto tiene como resultado, claro, que
     el recorrido total aumente (aumentando hasta los 326 para nuestra
     cadena de referencia.


*** Limitaciones de los algoritmos presentados

Ahora, mencionamos que estos algoritmos hoy en día ya casi no se
usan. Hay varias razones para ello. En primer término, todos ellos
están orientados a reducir el traslado /de la cabeza/, pero ignoran
la /demora rotacional/. Como vimos, en los discos duros actuales, la
demora rotacional va entre $1 \over 10$ y $1 \over 3$ del tiempo
total de recorrido de la cabeza. Y si bien el sistema podría
considerar esta demora como un factor adicional al planificar el
siguiente movimiento de forma que se redujera el tiempo de espera,
los algoritmos descritos obviamente requieren ser replanteados por
completo.

Por otro lado, el sistema operativo muchas veces requiere dar
distintas prioridades a los diferentes tipos de solicitud. Por
ejemplo, sería esperable dar preferencia a los accesos a memoria
virtual por encima de las solicitudes de abrir un nuevo archivo. Estos
algoritmos tampoco permiten expresar esta necesidad.

Pero el tercer punto es mucho más importante aún: Del mismo modo en
que los procesadores se van haciendo más rápidos y que la memoria
es cada vez de mayor capacidad, los controladores de discos también
son cada vez más /inteligentes/, y /esconden/ cada vez más
información del sistema operativo, por lo cual éste cada vez más
carece de la información necesaria acerca del acomodo /real/ de la
información como para planificar correctamente sus accesos.

Uno de los cambios más importantes en este sentido fue la transición
del empleo de la notación C-H-S al esquema de /direccionamiento lógico
de bloques/ (/Logical Block Addressing/, /LBA/) a principios de
los 1990. Hasta ese momento, el sistema operativo tenía información de
la ubicación /física/ de todos los bloques en el disco.

Una de las desventajas, sin embargo, de este esquema es que el mismo
BIOS tenía que conocer la /geometría/ de los discos — Y el BIOS
presentaba límites duros en este sentido: Principalmente, no le era
posible referenciar más allá de 64 cilindros. Al aparecer la interfaz
de discos IDE (/Electrónica integrada al dispositivo/) e ir
reemplazando a la ST-506, se introdujo LBA, que en un primer término
de la dirección C-H-S a una dirección /lineal/, presentando el disco
al sistema operativo ya no como un espacio /tridimensional/, sino que
como un gran arreglo de bloques. En este primer momento, la
equivalencia de una dirección C-H-S a una LBA era:

#+BEGIN_QUOTE
$LBA = ((C \times HPC) + H) \times SPT + S - 1$
#+END_QUOTE

Donde $HPC$ es el número de cabezas por cilindro, $SPT$ es el número
de sectores por pista.

LBA significó mucho más que una nueva notación — Marcó el inicio de
la transferencia de inteligencia y control del CPU al controlador de
disco. Podemos ver el impacto de esto directamente en dos factores:

- Sectores variables por cilindro :: En casi todos los discos previos
     a LBA[fn:: Como muy notorias excepciones tenemos a las unidades
     de disco /Commodore 1541/ y /Macintosh Superdrive/, que empleaban
     velocidad variable por cilindro para aprovechar mejor el medio
     magnético; en ambos casos, sin embargo, terminaron desapareciendo
     por cuestiones de costos y de complejidad al sistema], el número
     de sectores por pista se mantenía constante, se tratara de las
     pistas más internas o más externas. Esto significa que, a igual
     calidad de la cobertura magnética del medio, los sectores
     ubicados en la parte exterior del disco desperdiciaban mucho
     espacio (ya que el /área por bit/ era mucho mayor).

     #+attr_html: width="402" height="402"
     #+attr_latex: width=0.4\textwidth
     #+label: FS_FIS_zone_bit_recording
     #+caption: Disco formateado bajo /densidad de bits por zona/, con más sectores por pista en las pistas exteriores. (Imagen: Wikipedia)
     [[./img/zone_bit_recording.png]]

     Bajo LBA, los discos duros comenzaron a emplear un esquema de
     /densidad de bits por zona/ (/zone bit recording/), con la que en
     los cilindros más externos se aumenta.

- Reubicación de sectores :: Conforme avanza el uso de un disco, es
     posible que algunos sectores vayan resultando /difíciles/ de
     leer por daños microscópicos a la superficie. El controlador es
     capaz de detectar estos problemas, y de hecho, casi siempre
     puede rescatar la información de dichos sectores de forma
     imperceptible al usuario.

     Los discos duros ST-506 típicamente venían acompañados por una
     /lista de defectos/, una lista de coordenadas C-H-S que desde su
     fabricación habían presentado errores. El usuario debía ingresar
     estos defectos al formatear el disco /a bajo nivel/.

     Hoy en día, el controlador del disco detecta estos fallos y se
     los /salta/, presentando un mapa LBA lineal y completo. Los
     discos duros típicamente vienen con cierto número de /sectores de
     reserva/ para que, conforme se van detectando potenciales daños,
     estos puedan reemplazarse de forma transparente.

Sumemos a esto que los controladores de disco tienen ya incluso caché
para las operaciones de lectura y escritura — El controlador del disco
es hoy en día capaz de implementar estos mismos algoritmos de forma
completamente autónoma del sistema operativo.

Resulta claro que, dados estos cambios en la manera en que hacemos
referencia a los bloques del disco, el sistema operativo no cuenta ya
con la información necesaria para emplear los algoritmos de
planificación de acceso a disco.


** Almacenamiento en estado sólido
# <<FS_FIS_estado_solido>>

Desde hace cerca de una década va creciendo consistentemente el uso de
medios de almacenamiento de /estado sólido/ — Esto es, medios sin
partes móviles. Las características de estos medios de almacenamiento
son muy distintas de las de los discos. Si bien las estructuras que
emplean hoy en día prácticamente todos los sistemas de archivos en uso
mayoritario están pensadas y estructuradas siguiendo la lógica de los
medios magnéticos rotativos, la necesidad de emplear las estructuras
adecuadas es clara. Este es indudablemente un área bajo intensa
investigación y desarrollo, y que seguramente nos ofrecerá
importantes novedades en los próximos años.

Lo primero que llama la atención de estos medios de almacenamiento es
que, a pesar de ser fundamentalmente distintos a los discos
magnéticos, se presentan ante el sistema operativo como si fueran lo
mismo: En lo que podría entenderse como un esfuerzo para ser
utilizados pronto y sin cambios, se conectan a través de la misma
interfaz y empleando la misma semántica que un disco rotativo. Esto no
sólo evita que se aprovechen sus características únicas, adoptando
restricciones y criterios de diseño que ahora resultan indudablemente
artificiales, sino que incluso se exponen a mayor stress por no
emplearse de la forma que les resultaría natural. Antes de ver por
qué, veamos un poco los tipos de discos de estado solido que hay; esto
nos dará pie a entender las características a las que en este párrafo
hacemos referencia.

Al hablar de la tecnología sobre la cual se implementa este tipo de
almacenamiento, encontraremos dos medios principales:

- NVRAM :: Unidades /RAM No Volátil/. Almacenan la información en
           memoria RAM estándar, con un respaldo de batería para
           mantener la información cuando se desconecta la corriente
           externa. Las primeras unidades de estado sólido eran de
           este estilo; hoy en día son poco comunes en el mercado,
           pero siguen existiendo.

	   Su principal ventaja es la velocidad y durabilidad: El
           tiempo de acceso o escritura de datos es el mismo que el
           que esperaríamos de la memoria principal del sistema, y al
           no haber demoras mecánicas, este tiempo es el mismo
           independientemente de la dirección que se solicite.

	   Su principal desventaja es el precio: En líneas generales,
           la memoria RAM es, por volumen de almacenamiento, cientos
           de veces más cara que el medio magnético. Y si bien el
           medio no se degrada con el uso, la batería sí, lo que
           podría poner en peligro a la supervivencia de la
           información.

	   Estas unidades típicamente se instalan internamente como
           una tarjeta de expansión.

	   #+caption: Unidad de estado sólido basado en RAM: DDRdrive X1 ([[https://en.wikipedia.org/wiki/Solid-state\_drive][Imagen: Wikipedia]])
	   #+attr_latex: width=0.5\textwidth
	   #+attr_html: height="200" width="300"
	   [[./img/estado_solido_ddr_drivex1.jpg]]

- Memoria /flash/ :: Derivada de los /EEPROM/ (/Electrically Erasable
     Programmable Read-Only Memory/, /Memoria de Sólo Lectura
     Programable y Borrable Eléctricamente/). Los EEPROM tienen la
     característica de que, además de lectura y escritura, hay un
     tercer tipo de operación que deben implementar: El /borrado/. Un
     EEPROM ya utilizado debe borrarse antes de volverse a escribir a
     él. La principal característica que distingue a las memorias
     /flash/ de los EEPROMs tradicionales es que el espacio de
     almacenamiento está dividido en /páginas/ o /bloques/, y el
     controlador puede leer, borrar o escribir a cada uno de ellos por
     separado.

     El uso de dispositivos /flash/ para almacenamiento de información
     inició hacia 1995 como respuesta a las necesidades de las
     industrias aeroespacial y militar, dada la frecuencia de los
     daños a la información que presentaban los medios magnéticos por
     la vibración. Hoy en día hay dispositivos /flash/ de muy bajo
     costo y capacidad, aunque presentan una gran variabilidad tanto
     en su tiempo de acceso como en su durabilidad. En este sentido,
     podemos hablar de dos tipos principales de dispositivos /flash/:

  - Almacenamiento primario (SSD) :: Las llamadas formalmente
       /unidad de estado sólido/ (/Solid State Drive/)[fn:: Un
       error muy común es confundir la /D/ con /Disk/, que
       denotaría que llevan un /disco/, un /medio rotativo/] son
       unidades Flash de alta velocidad y capacidad, y típicamente
       presentan una interfaz similar a la que tienen los discos
       duros (hoy en día, la más común es SATA).

       #+caption: Unidad de estado sólido basado en Flash con interfaz SATA ([[https://en.wikipedia.org/wiki/Solid-state\_drive][Imagen: Wikipedia]])
       #+attr_html: height="262" width="400"
       #+attr_latex: width=0.5\textwidth
       [[./img/estado_solido_sata.jpg]]

       Su velocidad de lectura es muy superior y su velocidad de
       escritura (incluyendo el borrado) es comparable a la de los
       discos magnéticos. Su precio por el mismo volumen de
       almacenamento es entre 5 y 10 veces el de los discos
       magnéticos.

       Podemos encontrar este tipo de unidades tanto como unidades
       independientes en servidores, equipos de alto desempeño e
       incluso algunas subportátiles (/netbooks/) o como un
       componente de la tarjeta madre en dispositivos móviles como
       teléfonos y tabletas.

  - Transporte de archivos :: Esta tecnología también está presente en
       las diversas unidades extraíbles o móviles, como las unidades
       USB, SD, Memory Stick, Compact Flash, etc. La principal
       diferencia entre estas son los diferentes conectores que
       emplean; todas estas tecnologías presentan dispositivos que
       varían fuertemente en capacidad, velocidad y durabilidad.

       #+caption: Unidad de estado sólido basado en Flash con interfaz USB ([[https://en.wikipedia.org/wiki/Solid-state\_drive][Imagen: Wikipedia]])
       #+attr_html: height="300" width="335"
       #+attr_latex: width=0.5\textwidth
       [[./img/estado_solido_usb.jpg]]


Independientemente del tipo, las unidades de estado sólido presentan
ventajas ante los discos rotativos, como un muy bajo consumo
eléctrico, operación completamente silenciosa, y resistencia a la
vibración o a los golpes. Además, el medio es /verdaderamente/ de
acceso aleatorio: Al no ser ya un disco, desaparecen tanto la demora
de movimiento de cabezas como la rotacional.

*** Desgaste del medio

La memoria Flash presenta patrons de desgaste muy distintos de los que
podemos ver en otros medios. La memoria Flash tiene capacidad de
aguantar un cierto número de operaciones de borrado por página.[fn::
Dependiendo de la calidad, va entre las 3,000 y 100,000] Las
estructuras tradicionales de sistemas de archivos basados en disco
/concentran/ una gran cantidad de modificaciones en ciertas regiones
clave: Las tablas de asignación y directorios registran muchos más
cambios que la región de datos.

Casi todos los controladores de discos Flash cuentan con mecanismos de
/nivelamiento de escrituras/ (/write leveling/). Este mecanismo busca
reducir el desgaste focalizado modificando el mapeo de los sectores
que ve el sistema operativo respecto a los que son grabados /en
verdad/ en el medio: En vez de actualizar un bloque (por ejemplo, un
directorio) /en su lugar/, el controlador le asigna un nuevo bloque de
forma transparente, y marca el bloque original como libre.

Los mecanismos más simples de nivelamiento de escrituras lo hacen
únicamente intercambiando los bloques libres con los recién
reescritos; mecanismos más avanzados buscan nivelar el nivel de
reescritura en toda la unidad reubicando también a los bloques que
típicamente sólo se leen.

*** Emulación de discos

Hoy en día, la casi totalidad de medios de estado sóldo se presentan
ante el sistema con una interfaz que emula la de los discos, la /FTL/
(/Flash Translation Layer/, /Capa de Traducción de Flash/). La ventaja
de esta emulación es que no hizo falta desarrollar controladores
adicionales para comenzar a emplear estos medios. La desventaja, sin
embargo, es que al ocultarse el funcionamiento /real/ de las unidades
de estado sólido, el sistema operativo no puede aprovechar las
ventajas estructurales — Y más importante aún, no puede evitar las
debilidades inherentes al medio.

Uno de los ejemplos más claros de esta falta de control real del medio
la ilustra el [[https://lwn.net/Articles/353411/][artículo de Valerie Aurora (2009)]], que menciona que
tanto la poca información públicamente disponible acerca del
funcionamiento de los controladores como los patrones de velocidad y
desgaste de los mismos apuntan a que la estructura subyacente de casi
todos los medios de estado sólido es la de un /sistema de archivos
estructurado en bitácora/. Aurora indica que hay varias operaciones
que no pueden ser traducidas eficientemente a través de esta capa de
emulación, y que seguramente permitirían un mucho mejor
aprovechamiento del medio. Como mencionamos en la sección
\ref{FS_log_structured} (/Sistemas de archivo estructurados en
bitácora/), si bien varios de estos sistemas de archivos han
presentado implementaciones completamente utilizables, la falta de
interés ha llevado a que muchos de estos proyectos sean abandonados.

En su [[http://lwn.net/Articles/528617/][artículo de 2012]], Neil Brown apunta a que Linux tiene una
interfaz apta para hablar directamente con dispositivos de estado
sólido, llamada =mtd= — /memory technology devices/, /dispositivos de
tecnología de memoria/.

Como lo mencionamos al inicio de esta sección, si bien los discos
duros se han empleado por ya 50 años y los sistemas de archivos están
claramente desarrollados para aprovechar sus detalles físicos y
lógicos, el uso de los dispositivos de estado sólido apenas está
despegando en la última década. Y si bien esta primer aproximación que
nos permite emplear esta tecnologíá transparentemente es
/suficientemente buena/ para muchos de los usos básicos, sin duda hay
espacio para mejorar. Este es un tema que seguramente brinda amplio
espacio para investigación y desarrollo para los próximos años.

* RAID: Más allá de los límites físicos

En la sección \ref{FS_conceptos} se presentó muy escuetamente al
concepto de /volumen/. Mencionamos que un volumen /típicamente/
coincide con una partición, aunque no siempre es el caso — Y no
profundizamos más al respecto. En esta sección describiremos uno de
los mecanismos en que podemos combinar diferentes /dispositivos
físicos/ en un sólo volumen, y cómo –bajo las diferentes modalidades
que presentaremos– lleva a ganar en confiabilidad, rendimiento y
espacio disponible.

Abordaremos un esquema llamado /RAID/, /Arreglo Redundante de Discos
Baratos/ (/Redundant Array of Inexpensive Disks/)[fn:: Ocasionalmente
se presenta a RAID como acrónimo de /Arreglo Redundante de Discos
Independientes/ (/Redundant Array of Independent Disks/)], propuesto
en 1988 por David Patterson, Garth Gibson y Randy Katz ante el
diferencial que se presentaba (y se sigue presentando) entre el
avance en velocidad y confiabilidad del cómputo en relación al
almacenamiento magnético. Bajo los esquemas RAID queda sobreentendido
que los diferentes discos que forman parte de un volumen son del
mismo tamaño. Si se remplaza un disco de un arreglo por uno más
grande, la capacidad /en exceso/ que tenga éste sobre los demás
discos será desperdiciada.

Por muchos años, los arreglos RAID se hacían a través de controladores
dedicados, presentándose como un dispositivo único al sistema
operativo. Hoy en día, prácticamente todos los sistemas operativos
incluyen la capacidad de integrar varias unidades independientes en un
arreglo por software; esto conlleva un impacto en rendimiento, aunque
muy pequeño. Hay también, y abordaremos algunas de estas
implementaciones hacia el final de esta sección, varias
implementaciones derivadas de RAID que han incorporado conceptos de
capas superiores en algunos sistemas operativos modernos.

RAID no es un sólo esquema, sino que especifica un /conjunto/ de
/niveles/, cada uno de ellos diseñado para mejorar distintos aspectos
del almacenamiento en discos. Veamos las características de los
principales niveles en uso hoy en día.

** RAID nivel 0: División en /franjas/

Este esquema nos brinda una mejoría tanto en espacio total, dado que
presenta a un volumen grande en vez de varios discos probablemente más
pequeños, simplificando la tarea del administrador, como de velocidad,
dado que las lecturas y escrituras al volumen ya no estarán sujetas al
movimiento de una sola cabeza, sino que habrá una cabeza independiente
por cada uno de los discos que tengamos en el volumen.

#+attr_latex: width=0.7\textwidth
#+label: FS_FIS_raid_0
#+caption: Cinco discos organizados en RAID 0
[[./img/dot/raid_0.png]]

Los discos que participan en un volumen RAID 0 no son sencillamente
/concatenados/, sino que los datos son /divididos en franjas/ (en
inglés, el proceso se conoce como /striping/, de la palabra /stripe/,
franja; algunas traducciones al español se refieren a este proceso
como /bandeado/). Esto hace que la carga sea repartida de forma
uniforme entre todos los discos, y asegura que todas las
transferencias mayores al tamaño de una franja provengan de más de un
disco independiente.

#+attr_latex: width=0.6\textwidth
#+label: FS_FIS_franjas_raid_0
#+caption: División de datos en /franjas/
[[./img/ditaa/franjas_raid_0.png]]

La confiabilidad del volumen, sin embargo, disminuye
respecto a si cada uno de los discos se manejara por separado: Basta
con que uno de los discos presente daños para que la información
contenida en el volumen se pierda.

Puede construirse un arreglo bajo RAID nivel 0 con un mínimo de dos
discos.

** RAID nivel 1: Espejo

Este nivel está principalmente orientado a aumentar la confiabilidad
de la información: Los datos son grabados de forma /simultánea e
idéntica/ en todos los discos que formen parte del volumen. El costo
de mantener los datos en espejo, claro está, es el del espacio
empleado: En su configuración habitual, de dos discos por volumen, el
50% del espacio de almacenamiento se pierde por fungir como respaldo
del otro 50%.

La velocidad de acceso a los datos bajo RAID 1 es mayor a la que
tendríamos con un disco tradicional: Basta con que obtengamos la
respuesta de uno de los discos; el controlador RAID (sea el sistema
operativo o una implementación en hardware) puede incluso programar
las solicitudes de lectura para que se vayan repartiendo entre ambas
unidades. La velocidad de escritura se ve levemente reducida, dado
que hay que esperar a que ambos discos escriban la información.

#+label: FS_FIS_raid_1
#+caption: Dos discos organizados en RAID 1
#+attr_latex: width=0.3\textwidth
[[./img/dot/raid_1.png]]

Un arreglo RAID nivel 1 se construye típicamente con dos discos.

** Los niveles 2, 3 y 4 de RAID

Los siguientes tres niveles de RAID combinan propiedades de los
primeros junto con un algoritmo de verificación de integridad y
corrección de errores. Estos han caído casi por completo en el desuso
dado que los otros niveles, y muy en particular el nivel 5, ofrecen
las mismas características, pero con mayor confiabilidad

** RAID nivel 5: Paridad dividida por bloques

El nivel 5 de RAID proporciona un muy buen equilibrio respecto a las
características que venimos mencionando: nos brinda el espacio total
de almacenamiento de todos los discos que formen parte del volumen
/menos uno/. Para cada una de las /franjas/, RAID5 calcula un bloque
de /paridad/. Ahora, este bloque de paridad no siempre va al mismo
disco, sino que se va repartiendo entre todos los discos del volumen,
/desplazándose/ a cada franja, de modo que /cualquiera de los discos
puede fallar/, y el arreglo continuará operando sin pérdida de
información. Esta falla se notifica al administrador del sistema,
quien debe reemplazar el disco lo más pronto posible (dado que, de no
hacerlo, la falla en un segundo disco resultará en la pérdida de toda
la información).

#+attr_latex: width=0.7\textwidth
#+label: FS_FIS_franjas_raid_5
#+caption: División de datos en /franjas/, con paridad, para RAID 5
[[./img/ditaa/franjas_raid_5.png]]

Dependiendo de la configuración, la velocidad de acceso de este nivel
puede ser es ligeramente menor que la que obtendríamos de los discos
sin RAID, o ligeramente menor a la que obtendríamos con RAID
nivel 0. Dado que la electrónica en los discos actuales nos
notificará explícitamente en caso de fallo de lectura, al leer
información podemos leer únicamente los discos que contengan la
información (e ignorar al de paridad); si nuestro RAID está
configurado para verificar la paridad en lecturas, claro está, todas
las lecturas tendrán que obtener la franja correspondiente de todos
los discos del arreglo para poder calcularla.

Las escrituras son invariablemente más lentas que lo que obtenemos
tanto en ausencia de RAID como en niveles 0 y 1, dado que siempre
tendrá que recalcularse la paridad; en el caso de una escritura
mínima, menor a una franja, tendrá que leerse la franja entera de
todos los discos participantes en el arreglo, recalcularse la
paridad, y grabarse en el disco correspondiente.

Cuando uno de los discos falla, el arreglo comienza a trabajar en el
/modo interino de recuperación de datos/ (/Interim data recovery
mode/), en el que todas las lecturas involucran a todos los discos, ya
que tienen que estar recalculando y /rellenando/ la información que
provendría del disco dañado.

#+attr_latex: width=0.7\textwidth
#+label: FS_FIS_raid_5
#+caption: Cinco discos organizados en RAID 5
[[./img/dot/raid_5.png]]

Para implementar RAID nivel 5 son necesarios por lo menos 3 discos,
aunque es común verlos más /anchos/, pues de este modo se desperdicia
menos espacio en paridad. Si bien teóricamente un arreglo nivel 5
puede ser arbitrariamente ancho, en la práctica es muy raro ver
arreglos con más de 5 discos: Tener un arreglo más ancho aumentaría la
probabilidad de falla. Si un arreglo que está ya operando en el modo
interino de recuperación de datos se encuentra con una falla en
cualquiera de sus discos, tendrá que reportar un fallo irrecuperable.

** RAID nivel 6: Paridad por redundancia P+Q

Se trata nuevamente de un nivel de RAID muy poco utilizado. Se basa en
el mismo principio que el de RAID 5 pero, empleando dos distintos
algoritmos para calcular la paridad, permite la pérdida de hasta dos
de los discos del arreglo. La complejidad computacional es
sensiblemente mayor a la de RAID 5, no sólo porque se trata de un
segundo cálculo de paridad, sino porque este cálculo debe hacerse
empleando un algoritmo distinto y más robusto — Si bien para obtener
la paridad $P$ basta con hacer una operación /XOR/ sobre todos los
segmentos de una /franja/, la segunda paridad $Q$ típicamente emplea
al /algoritmo Reed-Solomon/, /paridad diagonal/ o /paridad dual
ortogonal/. Esto conlleva a una mayor carga al sistema, en caso de que
sea RAID por software, o a que el controlador sea de mayor costo por
implementar mayor complejidad, en caso de ser hardware dedicado.

#+attr_latex: width=0.7\textwidth
#+label: FS_FIS_raid_6
#+caption: Cinco discos organizados en RAID 6
[[./img/dot/raid_6.png]]

El nivel 6 de RAID puede implementarse con 4 o más unidades, y si
bien el espacio dedicado a la redundancia se incrementa a dos discos,
la redundancia adicional que ofrece este esquema permite crear
volúmenes con un mayor número de discos.

** Niveles combinados de RAID

Viendo desde el punto de vista de la abstracción presentada, RAID
toma una serie de dispositivos de bloques y los /combina/ en otro
dispositivo de bloques. Esto significa que puede tomarse una serie de
volúmenes RAID y combinarlos en uno solo, aprovechando las
características de los diferentes niveles.

Si bien pueden combinarse arreglos de todo tipo, hay combinaciones más
frecuentes que otras. Con mucho, la más popular es la de los niveles
1 + 0 — Esta combinación, frecuentemente llamada sencillamente /RAID
10/, ofrece un máximo de redundancia y rendimiento, sin sacrificar
demasiado espacio.

#+attr_latex: width=0.7\textwidth
#+label: FS_FIS_raid_10
#+caption: Seis discos organizados en RAID 1+0
[[./img/dot/raid_10.png]]

Con RAID nivel 10 creamos volúmenes que suman por franjas unidades en
espejo (un volumen RAID 0 compuesto de varios volúmenes RAID 1). En
caso de fallar cualquiera de las unidades del arreglo, ésta puede ser
reemplazada fácilmente, y su reemplazo no significará un trabajo tan
intensivo para el arreglo entero, sólo para su disco espejo.

Bajo este esquema, en el peor de los casos, en un volumen con $n$
discos físicos tendríamos $n \over 2$ volúmenes nivel 1, y por tanto
podríamos soportar la pérdida de hasta $n \over 2$ discos — Siempre
que estos no formen parte de un mismo volumen nivel 1.

Ahora bien, esta combinación nos ilustra cómo el órden de los
factores /sí altera/ el producto: Si en vez de la concatenación de
varias unidades espejeadas (un volumen nivel 0 compuesto de varios
volúmenes nivel 1) armáramos nuestro arreglo en órden inverso, esto
es, como el espejeo de varias unidades concatenadas por franjas,
parecería que obtenemos los mismos beneficios — Pero analizando lo
que ocurre en caso de falla, resulta claro que el nivel de
redundancia resulta mucho menor.

#+attr_latex: width=0.65\textwidth
#+label: FS_FIS_raid_01
#+caption: Seis discos organizados en RAID 0+1
[[./img/dot/raid_01.png]]

En este caso, nuestro arreglo soportará también el fallo de hasta $n
\over 2$ de sus discos, pero únicamente si ocurren en el mismo volumen
del espejo. Si se perdieran al mismo tiempo el disco 1 (del
subvolumen 1) y el disco 5 (del subvolumen 2), tendríamos pérdida de
datos.

* Manejo avanzado de volúmenes
# <<FS_FIS_man_av_vol>>

Los esquemas RAID vienen, sin embargo, de fines de la década de 1980,
y en los más de 20 años desde que fueron planteados, si bien han
cambiado el panorama del almacenamiento, han sido ya superados. En
nuestro caso, profundizamos en estos esquemas y no los desarrollos
posteriores dada la limpieza conceptual que presentan, y dado que
otros esquemas incluso hacen referencia al nivel de RAID que
/estarían reemplazando/ en su documentación.

Presentaremos brevemente a continuación algunos esquemas avanzados de
gestión de volúmenes, principalmente para ilustrar la dirección en que
parece ir avanzando la industria en este campo. Dado que no presentan
nuevos conceptos sino que sólo ilustran cómo se integran los que hemos
venido viendo en las últimas páginas, los expondremos meramente como
ejemplos de aplicación, sin entrar más que a un nivel meramente
descriptivo de su funcionamiento.

** LVM: el Gestor de Volúmenes Lógicos

Una evolución natural de los conceptos de RAID es la que Linux
presenta bajo el nombre /LVM2/ (segunda generación del /Logical Volume
Manager/, o /Gestor de Volúmenes Lógicos/). La lógica de operación de
LVM está basada en los siguientes conceptos:

- Volumen físico :: Cada uno de los discos o unidades disponibles
- Grupo de volúmenes :: Conjunto de volúmenes físicos que serán
     administrados como una sola entidad
- Volumen lógico :: Espacio dentro del grupo de volúmenes que se presenta
		    como un dispositivo, y que puede alojar sistemas
                    de archivos.

El esquema es limpio y elegante: LVM es una interfaz que permite, como
dos pasos independientes, agregar diferentes /volúmenes físicos/ a un
/grupo de volúmenes/, para posteriormente –y siguiendo las necesidades
del administrador del sistema, y ya no dependientes del tamaño de las
unidades físicamente existentes– crear las /unidades lógicas/, donde
se alojarán los sistemas de archivos propiamente.

Este esquema permite naturalmente una funcionalidad comparable con
RAID 0: Podemos crear un grupo de volúmenes con todos los discos que
tengamos a nuestra disposición, y dentro de este crear un volumen
lógico único. Dependiendo de la configuración, este volumen lógico
puede crecer abarcando todos los discos en cuestión, o dividirse en
franjas, logrando un mayor velocidad de acceso y un uso más uniforme.

Permite también la creación de unidades /espejo/, con una operación
a grandes rasgos equivalente a la de RAID1.

Para los niveles 4, 5 y 6 de RAID, la correspondencia es más directa
aún: Al crear un volumen, se le puede especificar a LVM al crear un
volumen lógico que buscamos crear un volumen con ese nivel de RAID —
Obviamente, siempre que contemos con suficientes volúmenes físicos.

El esquema de LVM no brinda, pues, funcionalidad distinta a la que
presenta RAID — Pero da al administrador del sistema flexibilidad:
ampliar o reducir el espacio dedicado a cada uno de los volúmenes,
incluso en un sistema en producción y con datos.

LVM ofrece varias funcionalidades adicionales, como las /fotografías/
(/snapshots/) o varios esquemas de reemplazo de disco; no
profundizaremos más en esta herramienta, pues no es el objetivo del
presente material.

** ZFS
# <<FS_FIS_zfs>>

Si bien LVM realiza una importante tarea de simplificación en la
administración del sistema, su operación sigue siendo orientada a
/bloques/: Los volúmenes lógicos deben aún ser formateados bajo el
sistema de archivos que el administrador del sistema considere acorde
para la tarea requerida.

ZFS[fn:: El nombre /ZFS/ proviene de /Zettabyte File System/, pues sus
estructuras son de 128 bits] fue desarrollado por Sun Microsystems
desde el año 2001, y forma parte del sistema operativo /Solaris/ desde
el 2005. Y si bien ZFS resulta suficientemente atractivo tan sólo por
haber sido diseñado para que el usuario nunca se tope con un límite
impuesto por el sistema operativo, el principal cambio que presenta al
usuario es una forma completamente distinta de referirse al
almacenamiento.

En primer término, al igual que LVM presentó una primer integración
entre conceptos, permitiendo unir de diferentes maneras varios
dispositivos físicos en un dispositivo lógico, ZFS incluye en la misma
lógica administrativa al sistema de archivos: En la configuración
estándar, basta conectar una unidad al sistema para que ésta aparezca
como espacio adicional disponible para los usuarios. El espacio
combinado de todas las unidades conforma un /fondo de almacenamiento/
(/storage pool/).

La lógica de ZFS parte de que operará una /colección/ de sistemas de
archivos en una organización jerárquica. Pero a diferencia del esquema
tradicional Unix en que cada sistema de archivos es preparado desde un
principio para su función, en ZFS se pueden aplicar límites a
jerarquías completas. Bajo un esquema ZFS, la creación y el montaje de
un sistema de archivos es una operación sencilla — Al grado que se
presenta como recomendación que, para cada usuario en el sistema, se
genere un sistema de archivos nuevo e independiente.

Una de las principales diferencias con los sistemas de archivos
tradicionales es el manejo del espacio vacío: El espacio disponible
total del fondo de almacenamiento se reporta como disponible /para
todos los sistemas de archivos/ que formen parte de éste. Sin embargo,
se pueden indicar /reservas/ (mantener un mínimo del espacio
especificado disponible para determinado subconjunto de sistemas de
archivos dentro de la colección) y /límites/ (evitar que el uso de una
colección exceda el almacenamiento indicado) para las necesidades de
las diferentes regiones del sistema.



* Otros recursos

- [[http://constantin.glez.de/blog/2010/03/opensolaris-zfs-deduplication-everything-you-need-know][OpenSolaris ZFS Deduplication: Everything You Need to Know]]
  (Constantin Gonzalez, 2010)

- [[http://www.freebsd.org/doc/en_US.ISO8859-1/books/handbook/filesystems-zfs.html][The Z File System (ZFS)]] (FreeBSD Handbook)

- [[http://www.cs.berkeley.edu/~brewer/cs262/LFS.pdf][The Design and Implementation of a Log-Structured File System]]
  (Mendel Rosenblum, J. K. Ousterhout, 1992)

- [[http://lwn.net/Articles/529077/][A hash-based DoS attack on Btrfs]] (LWN)

- [[http://linux.slashdot.org/story/12/12/15/0055217/denial-of-service-attack-found-in-btrfs-file-system][Denial-of-Service Attack Found In Btrfs File-System]] (Slashdot)

- [[http://delivery.acm.org/10.1145/150000/146943/p26-rosenblum.pdf][The Design and Implementation of a Log-Structured File System]] (John
  K. Ousterhout, Mendel Rosenblum, 1992)

- [[http://www.informatik.uni-osnabrueck.de/papers_pdf/2005_07.pdf][LogFS — Finally a scalable flash file system]] (Jörn Engel, Robert
  Mertens, 2005)

- [[https://lwn.net/Articles/353411/][Log-structured file systems: There's one in every SSD]] (Valerie
  Aurora, 2009)

- [[http://lwn.net/Articles/528617/][JFFS2, UBIFS, and the growth of flash storage]] (Neil Brown, 2012)

- [[http://www.cs.cmu.edu/%7Egarth/RAIDpaper/Patterson88.pdf][A case for Redundant Arrays of Inexpensive Disks]] (Patterson,
  Gibson, Katz 1988)

- [[http://insecurityit.blogspot.mx/2013/06/unidades-de-estado-solido-el-reto-de-la.html][Unidades de estado sólido. El reto de la computación forense en el mundo de los semiconductores]] (Cano Martinez, 2013)

- [[http://dx.doi.org/10.1038/ncomms2990][Non-volatile memory based on the ferroelectric photovoltaic effect]]
  (Guo, You, Zhow et. al., 2013)

- [[http://elinux.org/images/b/b6/EMMC-SSD_File_System_Tuning_Methodology_v1.0.pdf][eMMC/SSD File System Tuning Methodology]], Cogent Embedded, 2013
  (referido desde [[http://lwn.net/Articles/557220/][A flash filesystem tuning guide]], LWN, julio 2013)
