#+SETUPFILE: ../setup_notas.org
#+TITLE: El medio físico y el almacenamiento

* El medio físico
# <<FS_FIS>>

A lo largo del presente texto, particularmente de los capítulos
\ref{DIR} y \ref{FS} y siguiendo las prácticas a que ha impuesto la
realidad de los últimos 40 años, el término genérico de
/disco/ se ha empleado prácticamente como sinónimo de /medio de
almacenamiento a largo plazo/.

En este apéndice se abordan en primer término las características
principales del medio aún prevalente, los discos duros magnéticos
rotativos, y una introducción a las diferencias que presentan respecto
a otros medios, como los discos ópticos y los de estado sólido, así
como las implicaciones que éstos tienen sobre el material presentado
en el capítulo \ref{FS}.

Cabe mencionar que la razón de separar este contenido hacia un
apéndice es que, si bien estas funciones resultan relevantes para los
sistemas operativos y éstos cada vez más van asumiendo las funciones
que aquí serán descritas, éstas comenzaron siendo implementadas por
hardware especializado; fue apenas hasta la aparición de los esquemas
de manejo avanzado de volúmenes (que serán cubiertos en la sección
\ref{FS_FIS_man_av_vol}) que entran al ámbito del sistema operativo.

** Discos magnéticos rotativos

El principal medio de almacenamiento empleado en los últimos 40 años
es el /disco magnético/. Hay dos tipos diferentes de disco, aunque la
lógica de su funcionamiento es la misma: los /discos duros/ y los
/flexibles/ (o /floppies/).

La principal diferencia entre éstos es que, los primeros, son
típicamente almacenamiento /interno/ en los equipos de cómputo y, los
segundos, fueron pensados para ser almacenamiento /transportable/. Los
discos duros tienen mucha mayor capacidad y son mucho más rápidos,
pero a cambio de ello, son correspondientemente más sensibles a la
contaminación por partículas de polvo y a daños mecánicos, razón por
la cual hoy en día se venden, junto con el mecanismo lector e incluso
la electrónica de control, en empaque sellado.

Un disco flexible es una hoja de material plástico, muy similar al
empleado en las cintas magnéticas, resguardado por un estuche de
plástico. Al insertarse el disco en la unidad lectora, esta lo hace
girar sujetándolo por el centro, y las cabezas lectoras (en un
principio una sola; posteriormente aparecieron las unidades de doble
cara, con dos cabezas lectoras) se deslizan por una ventana que tiene
el estuche.

La mayor parte de los discos flexibles presentaban velocidades de
rotación de entre 300 y 400 revoluciones por minuto —presentaban,
pues, una /demora rotacional/ de entre 0.15 y 0.2 segundos. La /demora
rotacional/ es el tiempo que toma la cabeza lectora en volver a
posicionarse sobre un mismo sector del disco. (Véase la figura
\ref{FS_FIS_disco_duro})

A lo largo de más de 20 años se presentaron muy diferentes formatos
físicos siguiendo esta misma lógica, designándose principalmente por
su tamaño (en pulgadas). La capacidad de los discos, claro está, fue
creciendo con el paso de los años —esto explica la aparente
contradicción de que los discos (físicamente) más chicos tenían más
capacidad que los más grandes.

#+caption: Principales formatos de disco flexible que se popularizaron en el mercado.
|-----------------------+---------------------+---------------------+----------------------|
|                       |          8 pulgadas |       5.25 pulgadas |         3.5 pulgadas |
|-----------------------+---------------------+---------------------+----------------------|
| Fecha de introducción |                1971 |                1976 |                 1982 |
| Capacidad             | 150 *kb* a 1.2 *mb* | 110 *kb* a 1.2 *mb* | 264 *kb* a 2.88 *mb* |
| Velocidad (kbit/s)    |                  33 |             125-500 |             250-1000 |
| Pistas por pulgada    |                  48 |               48-96 |                  135 |
|-----------------------+---------------------+---------------------+----------------------|

El nombre de /disco duro/ o /disco flexible/ se debe al medio empleado
para el almacenamiento de la información (y no a la rigidez de su
/estuche/, como mucha gente erróneamente cree): mientras que los discos
flexibles emplean una hoja plástica flexible, los
duros son metálicos. Los discos están /permanentemente/
montados sobre un eje, lo que permite que tengan una velocidad de giro
entre 20 y 50 veces mayor que los discos flexibles — entre $4~200$ y
$15~000$ revoluciones por minuto (*rpm*), esto es, con una demora
rotacional de entre dos y 7.14 milisegundos.

Además, a excepción de algunos modelos tempranos, los discos duros
constituyen un paquete cerrado y sellado que incluye las cabezas de
lectura y escritura, y toda la electrónica de control. Esto permite
que los discos duros tengan densidades de almacenamiento y velocidades
de transmisión muy superiores a la de los discos flexibles: los
primeros discos duros que se comercializaron para computadoras
personales eran de 10 *mb* (aproximadamente 70 discos flexibles de su
época), y actualmente hay ya discos de 4 *tb*. La velocidad máxima de
transferencia sostenida hoy en día es superior a los 100 *mb* por
segundo, 100 veces más rápido que la última generación de discos
flexibles.

Para medir la eficiencia de un disco duro, además de la /demora
rotacional/ presentada unos párrafos atrás, el otro dato importante es
el tiempo que toma la cabeza en moverse por la superficie del
disco. Hoy en día, las velocidades más comunes son de 20 ms para un
/recorrido completo/ (desde el primer hasta el último sector), y entre
0.2 y 0.8 ms para ir de un cilindro al inmediato siguiente. Como punto
de comparación, el recorrido completo en una unidad de disco flexible
toma aproximadamente 100 ms, y el tiempo de un cilindro al siguiente
va entre 3 y 8 ms.

*** Notación C-H-S

En un principio, y hasta la década de los noventa, el sistema
operativo siempre hacía referencia a la ubicación de un bloque de
información en el disco empleando la /notación C-H-S/ — indicando el
cilindro, cabeza y sector (/Cylinder, Head, Sector/) para ubicar a
cada bloque de datos. Esto permite mapear el espacio de almacenamiento
de un disco a un espacio tridimensional, con el cual resulta trivial
ubicar un conjunto de datos en una región contigua.

#+attr_html: height="360" width="388"
#+attr_latex: width=0.5\textwidth
#+label: FS_FIS_disco_duro
#+caption: Coordenadas de un disco duro, ilustrando su geometría basada en cabeza, cilindro y sector (imagen de la Wikipedia: /Cilindro Cabeza Sector/).
[[./img/cilindro_cabeza_sector.png]]

La /cabeza/ indica a cuál de las superficies del disco se hace
referencia; en un disco flexible hay sólo una o dos cabezas (cuando
aparecieron las unidades de doble lado eran un lujo y, al paso de los
años, se fueron convirtiendo en la norma), pero en un disco duro es
común tener varios /platos/ paralelos. Todas las cabezas van fijas a
un mismo motor, por lo que no pueden moverse de forma independiente.

El /cilindro/ indica la distancia del centro a la orilla del disco. Al
cilindro también se le conoce como /pista/ (/track/), una metáfora
heredada de la época en que la música se distribuia principalmente en
discos de vinil, y se podía ver a simple vista la frontera entre una
pista y la siguiente.

Un /sector/ es un segmento de arco de uno de los cilindros y contiene
siempre la misma cantidad de información (históricamente 512 bytes;
actualmente se están adoptando gradualmente sectores de $4~096$
bytes. Refiérase a la sección \ref{FS_FIS_limitaciones} para una mayor
discusión al respecto.)

Un archivo almacenado secuencialmente ocupa /sectores adyacentes/ a lo
largo de una misma pista y con una misma cabeza.

*** Algoritmos de planificación de acceso a disco

Las transferencias desde y hacia los discos son uno de los procesos
más lentos de los que gestiona el sistema operativo. Cuando éste tiene
varias solicitudes de transferencia pendientes, resulta importante
encontrar un mecanismo óptimo para realizar la transferencia,
minimizando el tiempo de demora. A continuación se describirán a
grandes rasgos tres de los algoritmos históricos de planificación de
acceso a disco — para abordar después el por qué estos hoy en día casi
no son empleados.

Como con los demás escenarios en que se han abordado algoritmos, para
analizar su rendimiento, el análisis se realizará sobre una /cadena de
referencia/. Este ejemplo supone un disco hipotético de 200 cilindros,
la cadena de solicitudes /83, 175, 40, 120, 15, 121, 41, 42/, y
teniendo la cabeza al inicio de la operación en el cilindro 60.

En la figura \ref{FS_FIS_mov_cabeza_por_algoritmo} puede apreciarse de
forma gráfica la respuesta que presentarían los distintos algoritmos
ante la cadena de referencia dada.


#+attr_latex: width=0.9\textwidth
#+label: FS_FIS_mov_cabeza_por_algoritmo
#+caption: Movimientos de las cabezas bajo los diferentes algoritmos planificadores de acceso a disco, indicando la distancia total recorrida por la cabeza bajo cada uno, iniciando con la cabeza en la posición 60. Para *scan*, *look* y *c-scan*, se asume que la cabeza inicia avanzando en dirección decreciente.
[[./img/gnuplot/mov_cabeza_por_algoritmo.png]]

- *fifo* :: Del mismo modo que cuando fueron presentados los algoritmos
          de asignación de procesador y de reemplazo de páginas, el
          primero y más sencillo de implementar es el *fifo* —
          /primero llegado, primero servido/.

          Este algoritmo puede verse como muy
          /justo/, aunque sea muy poco eficiente: el movimiento total
          de cabezas para el caso planteado es de 622 cilindros,
          equivalente a poco más que recorrer de extremo a extremo el
          disco completo tres veces. Esto es, despreciando la demora
          rotacional la demora mecánica para que el brazo se detenga por
          completo antes de volver a moverse, esta lectura tomaría un
          mínimo de 60 ms, siendo el recorrido completo del disco 20 ms.

	  Puede identificarse como causante de buena parte de esta
          demora a la quinta posición de la cadena de referencia:
          entre solicitudes para los cilindros contiguos 120 y 121,
          llegó una solicitud al 15.

	  Atender esta solicitud en *fifo* significa un desplazamiento
          de $(120-15) + (121-15) = 211$ cilindros, para volver a
          quedar prácticamente en el mismo lugar de inicio. Una sola
          solicitud resulta responsable de la tercera parte del tiempo
          total.

- *sstf* :: Ahora bien, si el factor que impone la principal demora es
          el movimiento de la cabeza, el segundo algoritmo busca
          reducir al mínimo el movimiento de la cabeza: *sstf*
          (/Shortest Seek Time First/, /Tiempo de búsqueda más corto a
          continuación/) es el equivalente en este ámbito del
          /Proceso más corto a continuación/, presentado en la sección
          \ref{PLAN_spn} — con la ventaja de no estar prediciendo
          comportamiento futuro, sino partir de una lista de
          solicitudes pendientes. Empleando *sstf*, el tiempo de
          desplazamiento para este caso se reduce a tan sólo 207
          cilindros, muy cerca del mínimo absoluto posible.

	  Una desventaja de *sstf* es que puede llevar a la inanición:
          si hay una gran densidad de solicitudes para cilindros en
          determinada zona del disco, una solicitud para un cilindro
          alejado puede quedar a la espera
          indefinidamente.

	  Ejemplificando esto con una serie de solicitudes distinta a
          la cadena referencia: si el sistema tuviera que atender
          solicitudes por los cilindros /15, 175, 13, 20, 14, 32, 40,
          5, 6, 7/, *sstf* /penalizaría/ a la segunda solicitud (175)
          hasta terminar con los cilindros bajos. Si durante el tiempo
          que tome responder a estas solicitudes llegan otras
          adicionales, el proceso que está esperando el contenido del
          cilindro 175 puede quedar en espera indefinida.

- Familia de algoritmos /de elevador/ (*scan*, *look*, *c-scan*) :: En este
     tercer lugar se abordará ya no un sólo algoritmo, sino que una
     /familia/, dado que parten de la misma idea, pero con
     modificaciones menores llevan a que el patrón de atención
     resultante sea muy distinto.

     El planteamiento base para el algoritmo básico de elevador
     (*scan*) busca evitar la inanición, minimizando al mismo tiempo
     el movimiento de las cabezas. Su lógica indica que la cabeza debe
     recorrer el disco de un extremo a otro, como si fuera un elevador
     en un edificio alto, atendiendo a todas las solicitudes que haya
     pendientes en su camino. Si bien los recorridos para ciertos
     patrones pueden resultar en mayores desplazamientos a los que
     daría *sstf*, la garantía de que ningún proceso esperará
     indefinidamente lo hace muy atractivo.

     Atender la cadena de referencia bajo *scan*, asumiendo un estado
     inicial /descendente/ (esto es, la cabeza está en el cilindro 60
     y va bajando) da un recorrido total de 235 cilindros; empleando
     *look*, se reduce a 205 cilindros, y evita el movimiento
     innecesario hasta el límite del disco.

     Una primer (y casi obvia) modificación a este algoritmo sería,
     cada vez que la cabeza se detenga para satisfacer una solicitud,
     verificar si hay alguna otra solicitud pendiente en la /dirección
     actual/, y de no ser así, emprender el camino de regreso sin
     llegar a la orilla del disco. Esta modificación es frecuentemente
     descrita como *look*.

     Sin embargo, el patrón de atención a solicitudes de *scan* y *look*
     dejan qué desear: al llegar a un extremo del recorrido, es
     bastante probable que no haya ninguna solicitud pendiente en la
     primer mitad del recorrido de vuelta (dado que acaban de ser
     atendidas). El tiempo que demora atender a una solictud se
     compone de la suma del desplazamiento de la cabeza y la demora
     rotacional (que depende de cuál sector del cilindro fue
     solicitado). Para mantener una tasa de transferencia más
     predecible, el algoritmo *c-scan* (*scan* Circular) realiza las
     operaciones en el disco únicamente en un sentido — si el
     algoritmo lee en orden /descendente/, al llegar a la solicitud
     del cilindro más bajo, saltará de vuelta hasta el más alto para
     volver a iniciar desde ahí. Esto tiene como resultado, claro, que
     el recorrido total aumente (aumentando hasta los 339 para la
     cadena de referencia presentada).

*** Limitaciones de los algoritmos presentados
# <<FS_FIS_limitaciones>>

Ahora bien, ¿por qué se mencionó que estos algoritmos hoy en día ya
casi no se usan?

Hay varias razones. En primer término, todos estos algoritmos
están orientados a reducir el traslado /de la cabeza/, pero ignoran
la /demora rotacional/. Como se explicó, en los discos duros actuales, la
demora rotacional va entre $1 \over 10$ y $1 \over 3$ del tiempo
total de recorrido de la cabeza. Y si bien el sistema podría
considerar esta demora como un factor adicional al planificar el
siguiente movimiento de forma que se redujera el tiempo de espera,
los algoritmos descritos obviamente requieren ser replanteados por
completo.

Por otro lado, el sistema operativo muchas veces requiere dar
distintas prioridades a los diferentes tipos de solicitud. Por
ejemplo, se esperaría que diera preferencia a los accesos a memoria
virtual por encima de las solicitudes de abrir un nuevo archivo. Estos
algoritmos tampoco permiten expresar esta necesidad.

Pero el tercer punto es mucho más importante aún: del mismo modo
que los procesadores se van haciendo más rápidos y que la memoria
es cada vez de mayor capacidad, los controladores de discos también
son cada vez más /inteligentes/, y /esconden/ cada vez más
información del sistema operativo, por lo cual éste cada vez más
carece de la información necesaria acerca del acomodo /real/ de la
información como para planificar correctamente sus accesos.

Uno de los cambios más importantes en este sentido fue la transición
del empleo de la notación C-H-S al esquema de /direccionamiento lógico
de bloques/ (/Logical Block Addressing/, *lba*) a principios de los
noventa. Hasta ese momento, el sistema operativo tenía información de
la ubicación /física/ de todos los bloques en el disco.

Una de las desventajas, sin embargo, de este esquema es que hacía
necesario que el *bios* conociera la /geometría/ de los discos — y
éste presentaba límites duros en este sentido: principalmente, no le
era posible referenciar más allá de 64 cilindros. Al aparecer la
interfaz de discos *ide* (/electrónica integrada al dispositivo/) e ir
reemplazando a la *st-506*, se introdujo *lba*.

Este mecanismo convierte la dirección C-H-S a una dirección /lineal/,
presentando el disco al sistema operativo ya no como un espacio
/tridimensional/, sino que como un gran arreglo de bloques. En este
primer momento, partiendo de que $CPC$ denota el número de cabezas por
cilindro y $SPC$ el número de sectores por cilindro, la equivalencia
de una dirección C-H-S a una *lba* era:

#+BEGIN_QUOTE
$LBA = ((Cilindro \times CPC) + Cabeza) \times SPC + Sector - 1$
#+END_QUOTE

La transición de *chs* a *lba* significó mucho más que una nueva
notación: marcó el inicio de la transferencia de inteligencia y
control del *cpu* al controlador de disco. El efecto de esto se
refleja directamente en dos factores:

- Sectores variables por cilindro :: En casi todos los discos previos
     a *lba*,[fn:: Las unidades de disco /Commodore 1541/ y /Macintosh
     Superdrive/, que empleaban velocidad variable por cilindro para
     aprovechar mejor el medio magnético, constituyen notorias
     excepciones; en ambos casos, sin embargo, terminaron
     desapareciendo por cuestiones de costos y de complejidad al
     sistema.] el número de sectores por pista se mantenía constante,
     se tratara de las pistas más internas o más externas. Esto
     significa que, a igual calidad de la cobertura magnética del
     medio, los sectores ubicados en la parte exterior del disco
     desperdiciaban mucho espacio (ya que el /área por bit/ era mucho
     mayor).

     #+attr_html: width="402" height="402"
     #+attr_latex: width=0.5\textwidth
     #+label: FS_FIS_zone_bit_recording
     #+caption: Disco formateado bajo /densidad de bits por zona/, con más sectores por pista en las pistas exteriores (imagen de la Wikipedia: /Zone Bit Recording/).
     [[./img/zone_bit_recording.png]]

     Bajo *lba*, los discos duros comenzaron a emplear un esquema de
     /densidad de bits por zona/ (/zone bit recording/), con la que en
     los cilindros más externos se aumenta.

- Reubicación de sectores :: Conforme avanza el uso de un disco, es
     posible que algunos sectores vayan resultando /difíciles/ de
     leer por daños microscópicos a la superficie. El controlador es
     capaz de detectar estos problemas, y de hecho, casi siempre
     puede rescatar la información de dichos sectores de forma
     imperceptible al usuario.

     Los discos duros *st-506* típicamente iban acompañados por una
     /lista de defectos/, una lista de coordenadas C-H-S que desde su
     fabricación habían presentado errores. El usuario debía ingresar
     estos defectos al formatear el disco /a bajo nivel/.

     Hoy en día, el controlador del disco detecta estos fallos y se
     los /salta/, presentando un mapa *lba* lineal y completo. Los
     discos duros típicamente vienen con cierto número de /sectores de
     reserva/ para que, conforme se van detectando potenciales daños,
     estos puedan reemplazarse de forma transparente.

A estos factores se suma que a los controladores de disco se les
agregó también una memoria caché dedicada
para las operaciones de lectura y escritura. El controlador del disco
es hoy en día capaz de implementar estos mismos algoritmos de forma
completamente autónoma del sistema operativo.

Y si bien las diferentes unidades de disco duro habían mantenido
sectores de 512 bytes desde los primeros discos duros, a partir de la
aprobación del /Formato Avanzado/ en 2010 que incrementa los sectores
a $4~096$ bytes, presenta otra abstracción más: un disco con sectores
de $4~096$ bytes que es empleado por el sistema operativo como si
fuera de 512[fn:: Al día de hoy, los principales sistemas operativos
pueden ya hacer referencia al nuevo tamaño de bloque, pero la cantidad
de equipos que ejecutan sistemas /heredados/ o de controladores que no
permiten este nuevo modo de acceso limitan una adopción al 100 por
ciento.] tiene que efectuar, dentro de la lógica de su controlador,
una emulación — y una modificación de un solo sector se vuelve un
/ciclo lectura-modificación-escritura/ (*rmw*), que redunda en una
espera de por lo menos una revolución adicional (8 ms con un disco de
$7~200$ *rpm*) del disco antes de que la operación pueda completarse.

Resulta claro que, dados estos cambios en la manera en que debe
referirse a los bloques del disco, el sistema operativo no cuenta ya
con la información necesaria para emplear los algoritmos de
planificación de acceso a disco.

** Almacenamiento en estado sólido
# <<FS_FIS_estado_solido>>

Desde hace cerca de una década va creciendo consistentemente el uso de
medios de almacenamiento de /estado sólido/ — esto es, medios sin
partes móviles. Las características de estos medios de almacenamiento
son muy distintas de las de los discos.

Si bien las estructuras lógicas que emplean hoy en día prácticamente
todos los sistemas de archivos en uso mayoritario están pensadas
siguiendo la lógica de los medios magnéticos rotativos, como se verá
en esta sección, el empleo de estructuras más acordes a las
características del medio físico. Este es indudablemente un área bajo
intensa investigación y desarrollo, y que seguramente ofrecerá
importantes novedades en los próximos años.

Lo primero que llama la atención de estos medios de almacenamiento es
que, a pesar de ser fundamentalmente distintos a los discos
magnéticos, se presentan ante el sistema operativo como si fueran lo
mismo: en lo que podría entenderse como un esfuerzo para ser
utilizados pronto y sin tener que esperar a que los desarrolladores de
sistemas operativos adecuaran los controladores, se conectan mediante
la misma interfaz y empleando la misma semántica que un disco
rotativo.[fn:: Las unidades de estado sólido cuentan con una /capa de
traducción/ que emula el comportamiento de un disco duro, y presenta
la misma interfaz tanto de bus como semántica.] Esto no sólo evita que
se aprovechen sus características únicas, adoptando restricciones y
criterios de diseño que ahora resultan indudablemente artificiales,
sino que incluso se exponen a mayor /stress/ por no emplearse de la
forma que les resultaría natural.

Antes de ver por qué, conviene hacer un breve repaso de los tipos de
discos de estado solido que hay.  Al hablar de la tecnología sobre la
cual se implementa este tipo de almacenamiento, los principales medios
son:

- *nvram* :: Unidades *ram* No Volátil. Almacenan la información en
             chips de *ram* estándar, con un respaldo de batería para
             mantener la información cuando se desconecta la corriente
             externa. Las primeras unidades de estado sólido eran de
             este estilo; hoy en día son poco comunes en el mercado,
             pero todavía hay.

	     Su principal ventaja es la velocidad y durabilidad: el
             tiempo de acceso o escritura de datos es el mismo que el
             que podría esperarse de la memoria principal del sistema,
             y al no haber demoras mecánicas, este tiempo es el mismo
             independientemente de la dirección que se solicite.

	     Su principal desventaja es el precio: en líneas
             generales, la memoria *ram* es, por volumen de
             almacenamiento, cientos de veces más cara que el medio
             magnético. Y si bien el medio no se degrada con el uso,
             la batería sí, lo que podría poner en peligro a la
             supervivencia de la información.

	     Estas unidades típicamente se instalan internamente como
             una tarjeta de expansión.

	     #+caption: Unidad de estado sólido basado en *ram*: DDRdrive X1 (imagen de la Wikipedia: /Solid state drive/).
	     #+attr_latex: width=0.5\textwidth
	     #+attr_html: height="200" width="300"
	     [[./img/estado_solido_ddr_drivex1.jpg]]

- Memoria /flash/ :: Derivada de los *eeprom* (/Electrically Erasable
     Programmable Read-Only Memory/, /Memoria de Sólo Lectura
     Programable y Borrable Eléctricamente/). Los *eeprom* tienen la
     característica de que, además de lectura y escritura, hay un
     tercer tipo de operación que deben implementar: el /borrado/. Un
     *eeprom* ya utilizado debe borrarse antes de volverse a escribir a
     él. La principal característica que distingue a las memorias
     /flash/ de los *eeprom* tradicionales es que el espacio de
     almacenamiento está dividido en muchas /celdas/, y el controlador
     puede leer, borrar o escribir a cada uno de ellos por
     separado.[fn:: Estos dispositivos se conocen como /flash/ en
     referencia a los chips *eprom* (antes de que fuera posible borrar
     /eléctricamente/): estos chips tenían una ventana en la parte
     superior, y debían operar siempre cubiertos con una
     etiqueta. Para borrar sus contenidos, se retiraba la etiqueta y
     se les administraba una descarga lumínica — un /flash/.]

     El uso de dispositivos /flash/ para almacenamiento de información
     inició hacia 1995 como respuesta a las necesidades de las
     industrias aeroespacial y militar, dada la frecuencia de los
     daños a la información que presentaban los medios magnéticos por
     la vibración. Hoy en día hay dispositivos /flash/ de muy bajo
     costo y capacidad, aunque presentan una gran variabilidad tanto
     en su tiempo de acceso como en su durabilidad. En este sentido,
     hay dos tipos principales de dispositivos /flash/:

  - Almacenamiento primario (*ssd*) :: Las llamadas formalmente
       /unidad de estado sólido/ (/Solid State Drive/)[fn:: Un
       error muy común es confundir la /D/ con /Disk/, que
       denotaría que llevan un /disco/, un /medio rotativo/.] son
       unidades Flash de alta velocidad y capacidad, y típicamente
       presentan una interfaz similar a la que tienen los discos
       duros; hoy en día, la más común es *sata*.

       #+caption: Unidad de estado sólido basado en Flash con interfaz *sata* (imagen de la Wikipedia: /Solid state drive/).
       #+attr_html: height="262" width="400"
       #+attr_latex: width=0.5\textwidth
       [[./img/estado_solido_sata.jpg]]

       Su velocidad de lectura es muy superior y su velocidad de
       escritura (incluyendo el borrado) es comparable a la de los
       discos magnéticos. Su precio por el mismo volumen de
       almacenamento es entre 5 y 10 veces el de los discos
       magnéticos.

       Estas unidades se emplean tanto como unidades
       independientes en servidores, equipos de alto desempeño e
       incluso algunas subportátiles (/netbooks/) o como un
       componente de la tarjeta madre en dispositivos móviles como
       teléfonos y tabletas.

  - Transporte de archivos :: Esta tecnología también está presente en
       las diversas unidades extraíbles o móviles, como las unidades
       *usb*, *sd*, Memory Stick, Compact Flash, etc. La principal
       diferencia entre estas son los diferentes conectores que
       emplean; todas estas tecnologías presentan dispositivos que
       varían fuertemente en capacidad, velocidad y durabilidad.

       #+caption: Unidad de estado sólido basado en Flash con interfaz *usb* (imagen de la Wikipedia: /Solid state drive/).
       #+attr_html: height="300" width="335"
       #+attr_latex: width=0.5\textwidth
       [[./img/estado_solido_usb.png]]


Independientemente del tipo, las unidades de estado sólido presentan
ventajas ante los discos rotativos, como un muy bajo consumo
eléctrico, operación completamente silenciosa, y resistencia a la
vibración o a los golpes. Además, el medio es /verdaderamente/ de
acceso aleatorio: al no ser ya un disco, desaparecen tanto la demora
de movimiento de cabezas como la rotacional.

*** Desgaste del medio

La memoria Flash presenta patrones de desgaste muy distintos de los que
presentan otros medios. La memoria Flash tiene capacidad de
aguantar un cierto número de operaciones de borrado por página[fn::
Dependiendo de la calidad, va entre las $3~000$ y $100~000$.] antes de
comenzar a degradarse y fallar. Las
estructuras tradicionales de sistemas de archivos basados en disco
/concentran/ una gran cantidad de modificaciones frecuentes a lo largo
de la operación normal del sistema en ciertas regiones
clave: las tablas de asignación y directorios registran muchos más
cambios que la región de datos.

Casi todos los controladores de discos Flash cuentan con mecanismos de
/nivelamiento de escrituras/ (/write leveling/). Este mecanismo busca
reducir el desgaste focalizado modificando el mapeo de los sectores
que ve el sistema operativo respecto a los que son grabados /en
verdad/ en el medio: en vez de actualizar un bloque (por ejemplo, un
directorio) /en su lugar/, el controlador le asigna un nuevo bloque de
forma transparente, y marca el bloque original como libre.

Los mecanismos más simples de nivelamiento de escrituras lo hacen
únicamente intercambiando los bloques libres con los recién
reescritos; mecanismos más avanzados buscan equilibrar el nivel de
reescritura en toda la unidad reubicando periódicamente también a los
bloques que no son modificados, para no favorecerlos injustamente y
hacer un mejor balanceo de uso.

*** Emulación de discos

Hoy en día, casi la totalidad de medios de estado sólido se presentan
ante el sistema con una interfaz que emula la de los discos, la *ftl*
(/Flash Translation Layer/, /Capa de Traducción de Flash/). La ventaja
de esta emulación es que no hizo falta desarrollar controladores
adicionales para comenzar a emplear estos medios. La desventaja, sin
embargo, es que al ocultarse el funcionamiento /real/ de las unidades
de estado sólido, el sistema operativo no puede aprovechar las
ventajas estructurales — y más importante aún, no puede evitar las
debilidades inherentes al medio.

Uno de los ejemplos más claros de esta falta de control real del medio
la ilustra \parencite{Aurora2009}, que menciona que
tanto la poca información públicamente disponible acerca del
funcionamiento de los controladores como los patrones de velocidad y
desgaste de los mismos apuntan a que la estructura subyacente de casi
todos los medios de estado sólido es la de un /sistema de archivos
estructurado en bitácora/. Aurora indica que hay varias operaciones
que no pueden ser traducidas eficientemente por medio de esta capa de
emulación, y que seguramente permitirían un mucho mejor
aprovechamiento del medio. Como se mencionó en la sección
\ref{FS_log_structured} (/Sistemas de archivo estructurados en
bitácora/), si bien varios de estos sistemas de archivos han
presentado implementaciones completamente utilizables, la falta de
interés ha llevado a que muchos de estos proyectos sean abandonados.

En su [[http://lwn.net/Articles/528617/][artículo de 2012]], Neil Brown apunta a que Linux tiene una
interfaz apta para hablar directamente con dispositivos de estado
sólido, llamada =mtd= — /memory technology devices/, /dispositivos de
tecnología de memoria/.

Si bien los discos duros se han empleado por ya 50 años y los sistemas
de archivos están claramente desarrollados para aprovechar sus
detalles físicos y lógicos, el uso de los dispositivos de estado
sólido apenas está despegando en la última década. Y si bien esta
primer aproximación que permite emplear esta tecnología
transparentemente es /suficientemente buena/ para muchos de los usos
básicos, sin duda hay espacio para mejorar. Este es un tema que
seguramente brinda amplio espacio para investigación y desarrollo para
los próximos años.

* *raid*: Más allá de los límites físicos
# <<FS_FIS_RAID>>
En la sección \ref{FS_conceptos} se presentó muy escuetamente al
concepto de /volumen/, mencionando que un volumen /típicamente/
coincide con una partición, aunque no siempre es el caso — sin
profundizar más al respecto. En esta sección se presentará uno de los
mecanismos que permite combinar diferentes /dispositivos físicos/ en
un sólo volumen, llevando –bajo sus diferentes modalidades– a mayor
confiabilidad, rendimiento y espacio disponible.

El esquema más difundido para este fin es conocido como *raid*,
/Arreglo Redundante de Discos Baratos/ (/Redundant Array of
Inexpensive Disks/),[fn:: Ocasionalmente se presenta a *raid* como
acrónimo de /Arreglo Redundante de Discos Independientes/ (/Redundant
Array of Independent Disks/)] propuesto en 1988 por David Patterson,
Garth Gibson y Randy Katz ante el diferencial que se presentaba (y se
sigue presentando) entre el avance en velocidad y confiabilidad de las
diversas áreas del cómputo en relación con el almacenamiento magnético.

Bajo los esquemas *raid* queda sobreentendido que los diferentes discos
que forman parte de un volumen son del mismo tamaño. Si se reemplaza un
disco de un arreglo por uno más grande, la capacidad /en exceso/ que
tenga éste sobre los demás discos será desperdiciada.

Por muchos años, para emplear un /arreglo/ *raid* era necesario contar
con controladores dedicados, que presentaban al conjunto como un
dispositivo único al sistema operativo. Hoy en día, prácticamente
todos los sistemas operativos incluyen la capacidad de integrar varias
unidades independientes en un arreglo por software; esto conlleva un
impacto en rendimiento, aunque muy pequeño. Hay también varias
tecnologías presentes en distintos sistemas operativos modernos que
heredan las ideas presentadas por *raid*, pero integrándolos con
funciones formalmente implementadas por capas superiores.

La operación de *raid*, más que un único esquema, especifica un
/conjunto/ de /niveles/, cada uno de ellos diseñado para mejorar
distintos aspectos del almacenamiento en discos. Se exponen a
continuación las características de los principales niveles en uso hoy
en día.

** *raid* nivel 0: división en /franjas/

El primer nivel de *raid* brinda una ganancia tanto en espacio total,
dado que presenta un volumen grande en vez de varios discos más
pequeños (simplificando la tarea del administrador) como de velocidad,
dado que las lecturas y escrituras al volumen ya no estarán sujetas al
movimiento de una sola cabeza, sino que habrá una cabeza independiente
por cada uno de los discos que conformen al volumen.

#+attr_latex: width=0.7\textwidth
#+label: FS_FIS_raid_0
#+caption: Cinco discos organizados en *raid* 0.
[[./img/dot/raid_0.png]]

Los discos que participan en un volumen *raid* 0 no están sencillamente
/concatenados/, sino que los datos son /divididos en franjas/ (en
inglés, el proceso se conoce como /striping/, de la palabra /stripe/,
franja; algunas traducciones al español se refieren a este proceso
como /bandeado/). Esto hace que la carga se reparta de forma
uniforme entre todos los discos, y asegura que todas las
transferencias mayores al tamaño de una franja provengan de más de un
disco independiente.

#+attr_latex: width=0.6\textwidth
#+label: FS_FIS_franjas_raid_0
#+caption: División de datos en /franjas/.
[[./img/ditaa/franjas_raid_0.png]]

La confiabilidad del volumen, sin embargo, disminuye
respecto a si cada uno de los discos se manejara por separado: basta
con que uno de los discos presente daños para que la información
contenida en el volumen se pierda.

Un arreglo *raid* nivel 0 puede construirse con un mínimo de dos discos.

** *raid* nivel 1: espejo

Este nivel está principalmente orientado a aumentar la confiabilidad
de la información: los datos son grabados de forma /simultánea e
idéntica/ en todos los discos que formen parte del volumen. El costo
de mantener los datos en espejo, claro está, es el del espacio
empleado: en su configuración habitual, de dos discos por volumen, 50%
del espacio de almacenamiento se pierde por fungir como respaldo del
otro 50 por ciento.

La velocidad de acceso a los datos bajo *raid* 1 es mayor a la que se
lograría con un disco tradicional: basta con obtener los datos de uno
de los discos; el controlador *raid* (sea el sistema operativo o una
implementación en hardware) puede incluso programar las solicitudes de
lectura para que se vayan repartiendo entre ambas unidades. La
velocidad de escritura se ve levemente reducida, dado que hay que
esperar a que ambos discos escriban la información.

#+label: FS_FIS_raid_1
#+caption: Dos discos en espejo con *raid* 1.
#+attr_latex: width=0.3\textwidth
[[./img/dot/raid_1.png]]

Un arreglo *raid* nivel 1 se construye típicamente con dos discos.

** Los niveles 2, 3 y 4 de *raid*

Los siguientes tres niveles de *raid* combinan propiedades de los
primeros junto con un algoritmo de verificación de integridad y
corrección de errores. Estos han caído casi por completo en el desuso
dado que los otros niveles, y muy en particular el nivel 5, ofrecen
las mismas características, pero con mayor confiabilidad

** *raid* nivel 5: paridad dividida por bloques

El nivel 5 de *raid* proporciona un muy buen equilibrio respecto a las
características que se han mencionando: brinda el espacio total
de almacenamiento de todos los discos que formen parte del volumen
/menos uno/. Para cada una de las /franjas/, *raid* 5 calcula un bloque
de /paridad/.

Para obtener una mayor tolerancia a fallos, este bloque de paridad no siempre va al mismo
disco, sino que se va repartiendo entre todos los discos del volumen,
/desplazándose/ a cada franja, de modo que /cualquiera de los discos
puede fallar/, y el arreglo continuará operando sin pérdida de
información. Esta debe notificarse al administrador del sistema,
quien reemplazará al disco dañado lo antes posible (dado que, de no
hacerlo, la falla en un segundo disco resultará en la pérdida de toda
la información).

En equipos *raid* profesionales es común contar con discos de reserva
/en caliente/ (/hot spares/): discos que se mantienen apagados pero
listos para trabajar. Si el controlador detecta un disco dañado, sin
esperar a la intervención del administrador, desactiva al disco
afectado y activa al /hot spare/, reconstruyendo de inmediato la
información a partir de los datos en los discos /sanos/.

#+attr_latex: width=0.9\textwidth
#+label: FS_FIS_franjas_raid_5
#+caption: División de datos en /franjas/, con paridad, para *raid* 5.
[[./img/ditaa/franjas_raid_5.png]]

Dependiendo de la configuración, la velocidad de acceso de este nivel
puede ser es ligeramente menor que la obtenida de los discos que no
operan en *raid*, o ligeramente menor a la que se logra con *raid*
nivel 0. Dado que la electrónica en los discos actuales notificará
explícitamente al sistema operativo en caso de fallo de lectura,
cuando el sistema requiere leer datos, estos pueden ser solicitados
únicamente a $n-1$ discos (e ignorar al de paridad); si el arreglo
está configurado para verificar la paridad en lecturas, todas tendrán
que obtener la franja correspondiente de todos los discos del arreglo
para poder calcularla.

El algoritmo de verificación y recuperación de *raid* 5 es
sorprendentemente eficiente y simple: el de una
#+latex: \emph{suma \textbf{xor}},
#+html: <i>suma <b>xor</b></i>,
ilustrado en la figura \ref{FS_FIS_redundancia_xor}. La operación
booleana *xor*
#+latex: (de \emph{Exclusive \textbf{or}})
#+html: (de <i>Exclusive <b>or</b></i>)
suma los bits individuales, columna por columna. Si es un número par,
almacena un 0, si es impar, almacena un 1. Esta operación es muy
eficiente computacionalmente.

#+attr_latex: width=0.7\textwidth
#+label: FS_FIS_redundancia_xor
#+caption: Para cada franja, el disco de paridad guarda la suma *xor* de los bits de las franjas correspondientes de los otros discos; no importa cuál disco falle, sus datos pueden recuperarse haciendo un *xor* de los datos de los demás.
[[./img/ditaa/redundancia_xor.png]]

Las escrituras son invariablemente más lentas respecto tanto ante
la ausencia de *raid* como en niveles 0 y 1, dado que siempre
tendrá que recalcularse la paridad; en el caso de una escritura
mínima (menor a una franja) tendrá que leerse la franja entera de
todos los discos participantes en el arreglo, recalcularse la
paridad, y grabarse en el disco correspondiente.

Cuando uno de los discos falla, el arreglo comienza a trabajar en el
/modo interino de recuperación de datos/ (/Interim data recovery
mode/, también conocido como /modo degradado/), en el que todas las lecturas involucran a todos los discos, ya
que tienen que estar recalculando y /rellenando/ la información que
provendría del disco dañado.

#+attr_latex: width=0.8\textwidth
#+label: FS_FIS_raid_5
#+caption: Cinco discos organizados en *raid* 5. La franja de paridad se va alternando, repartiéndose entre todos los discos.
[[./img/dot/raid_5.png]]

Para implementar *raid* nivel 5 son necesarios por lo menos tres
discos, aunque es común verlos más /anchos/, pues de este modo se
desperdicia menos espacio en paridad. Si bien teóricamente un arreglo
nivel 5 puede ser arbitrariamente ancho, en la práctica es muy raro
ver arreglos con más de cinco discos: tener un arreglo más ancho
aumentaría la probabilidad de falla. Si un arreglo que está ya
operando en el modo interino de recuperación de datos se encuentra con
una falla en cualquiera de sus discos, tendrá que reportar un fallo
irrecuperable.

** *raid* nivel 6: paridad por redundancia P+Q

Se trata nuevamente de un nivel de *raid* muy poco utilizado. Se basa en
el mismo principio que el de *raid* 5 pero, empleando dos distintos
algoritmos para calcular la paridad, permite la pérdida de hasta dos
de los discos del arreglo. La complejidad computacional es
sensiblemente mayor a la de *raid* 5, no sólo porque se trata de un
segundo cálculo de paridad, sino porque este cálculo debe hacerse
empleando un algoritmo distinto y más robusto — si bien para obtener
la paridad $P$ basta con hacer una operación *xor* sobre todos los
segmentos de una /franja/, la segunda paridad $Q$ típicamente emplea
al /algoritmo Reed-Solomon/, /paridad diagonal/ o /paridad dual
ortogonal/. Esto conlleva a una mayor carga al sistema, en caso de que
sea *raid* por software, o a que el controlador sea de mayor costo por
implementar mayor complejidad, en caso de ser hardware dedicado.

#+attr_latex: width=0.7\textwidth
#+label: FS_FIS_raid_6
#+caption: Cinco discos organizados en *raid* 6. Al igual que bajo *raid* 5, las franjas de paridad se van alternando entre todos los discos.
[[./img/dot/raid_6.png]]

El nivel 6 de *raid* puede implementarse con cuatro o más unidades, y
si bien el espacio dedicado a la redundancia se incrementa a dos
discos, la redundancia adicional que ofrece este esquema permite crear
volúmenes con un mayor número de discos.

** Niveles combinados de *raid*

Viendo desde el punto de vista de la abstracción presentada, *raid*
toma una serie de dispositivos de bloques y los /combina/ en otro
dispositivo de bloques. Esto significa que puede tomarse una serie de
volúmenes *raid* y combinarlos en uno solo, aprovechando las
características de los diferentes niveles.

Si bien pueden combinarse arreglos de todo tipo, hay combinaciones más
frecuentes que otras. Con mucho, la más popular es la de los niveles
1 + 0 — esta combinación, frecuentemente llamada sencillamente *raid*
10, ofrece un máximo de redundancia y rendimiento, sin sacrificar
demasiado espacio.

#+attr_latex: width=0.7\textwidth
#+label: FS_FIS_raid_10
#+caption: Seis discos organizados en *raid* 1+0.
[[./img/dot/raid_10.png]]

Con *raid* nivel 10 se crean volúmenes que suman por franjas unidades
en espejo (un volumen *raid* 0 compuesto de varios volúmenes *raid*
1). En caso de fallar cualquiera de las unidades del arreglo, ésta
puede ser reemplazada fácilmente, operación que no significará un
trabajo tan intensivo para el arreglo entero (sólo para su disco
espejo).

Bajo este esquema, en el peor de los casos, un volumen con $n$
discos físicos está conformado por $n \over 2$ volúmenes nivel 1, y por tanto
puede soportar la pérdida de hasta $n \over 2$ discos — siempre
que estos no formen parte de un mismo volumen nivel 1.

Esta combinación ilustra cómo el orden de los
factores /sí altera/ el producto: si en vez de la concatenación de
varias unidades espejeadas (un volumen nivel 0 compuesto de varios
volúmenes nivel 1) se armara el arreglo en orden inverso (esto
es, como el espejeo de varias unidades concatenadas por franjas),
ante un primer análisis parecería se obtienen los mismos beneficios —
pero analizando lo que ocurre en caso de falla, resulta claro que el
nivel de redundancia resulta mucho menor.

#+attr_latex: width=0.65\textwidth
#+label: FS_FIS_raid_01
#+caption: Seis discos organizados en *raid* 0+1, ilustrando cómo una combinación errónea puede /reducir/ la tolerancia máxima a fallos del arreglo.
[[./img/dot/raid_01.png]]

En este caso, el arreglo soportará también el fallo de hasta $n \over
2$ de sus discos, pero únicamente si ocurren /en el mismo volumen *raid*
1/ del espejo.

Dado que *raid* opera meramente agregando /dispositivos de bloques/ en
un nuevo dispositivo del mismo tipo, no tiene conocimiento de la
información subyacente. Por tanto, si se perdieran al mismo tiempo el
disco 1 (del subvolumen 1) y el disco 5 (del subvolumen 2), resultaría
en pérdida de datos.[fn:: O por lo menos, en una tarea de
reconstrucción manual, dada que la información completa puede aún ser
encontrada. Sin embargo, ambos volúmenes del arreglo *raid* 1 estarían
dañados e incompletos.]

* Manejo avanzado de volúmenes
# <<FS_FIS_man_av_vol>>

Los esquemas *raid* vienen, sin embargo, de finales de la década de
los ochenta, y si bien han cambiado el panorama del almacenamiento, en
los más de 20 años desde su aparición, han sido ya superados. El
énfasis en el estudio de *raid* (y no tanto en los desarrollos
posteriores) se justifica dada la limpieza conceptual que presentan, y
dado que esquemas posteriores incluso hacen referencia explícita en su
documentación al nivel de *raid* que /estarían reemplazando/.

A continuación se presentan brevemente dos esquemas avanzados de
gestión de volúmenes, principalmente ilustrando la dirección en que
parece ir avanzando la industria en este campo. Dado que no presentan
nuevos conceptos sino que sólo ilustran cómo se integran los que se
han expuesto en las últimas páginas, la exposición se limitará a
presentar ejemplos de aplicación, sin entrar más que a un nivel
descriptivo de su funcionamiento.

** *lvm:* el Gestor de Volúmenes Lógicos

Una evolución natural de los conceptos de *raid* es el *lvm2* (segunda
generación del /Logical Volume Manager/, o /Gestor de Volúmenes
Lógicos/) de Linux. La lógica de operación de *lvm* está basada en los
siguientes conceptos:

- Volumen físico :: Cada uno de los discos o unidades disponibles.
- Grupo de volúmenes :: Conjunto de volúmenes físicos que serán
     administrados como una sola entidad.
- Volumen lógico :: Espacio dentro del grupo de volúmenes que se presenta
		    como un dispositivo, y que puede alojar sistemas
                    de archivos.

El esquema es limpio y elegante: *lvm* es una interfaz que permite, como
dos pasos independientes, agregar diferentes /volúmenes físicos/ a un
/grupo de volúmenes/, para posteriormente –y siguiendo las necesidades
del administrador del sistema, ya independientes del tamaño de las
unidades físicamente existentes– crear las /unidades lógicas/, donde
se alojarán los sistemas de archivos propiamente.

Este esquema permite naturalmente una funcionalidad comparable con la
de un *raid* 0: puede crearse un grupo de volúmenes con todos los
discos que disponibles, y dentro de este crear un volumen lógico
único. Dependiendo de la configuración, este volumen lógico puede
crecer abarcando todos los discos en cuestión, sea como simple
concatenación o dividiéndose en franjas.

Permite también la creación de unidades /espejo/, con una operación
a grandes rasgos equivalente a la de *raid* 1. Incluso, dentro de un
mismo grupo de volúmenes, pueden haber tanto volúmenes lógicos
espejeados como otros que no lo estén, a diferencia de la estricta
rigidez de *raid*.

Para los niveles 4, 5 y 6 de *raid*, la correspondencia es más directa
aún: al crear un volumen, se le puede solicitar a *lvm* al crear un
volumen lógico que cree un volumen con ese nivel de *raid* — obviamente,
siempre que cuente con suficientes volúmenes físicos.

El esquema de *lvm* no brinda, pues, funcionalidad estrictamente
distinta a la que presenta *raid* — pero da al administrador del sistema
flexibilidad: ampliar o reducir el espacio dedicado a cada uno de los
volúmenes, incluso en un sistema en producción y con datos.

Además de lo anterior, *lvm* ofrece varias funciones adicionales, como
las /fotografías/ (/snapshots/) o varios esquemas de reemplazo de
disco; si bien hay mucho más que podría decirse de *lvm*, no se
profundiza más en esta herramienta dado que excede del objetivo del
presente material.

** *zfs*
# <<FS_FIS_zfs>>

Si bien *lvm* realiza una importante tarea de simplificación en la
administración del sistema, su operación sigue siendo orientada a
/bloques/: los volúmenes lógicos deben aún ser formateados bajo el
sistema de archivos que el administrador del sistema considere acorde
para la tarea requerida.

El sistema de archivos *zfs*[fn:: El nombre *zfs* proviene de
/Zettabyte File System/. Los diseñadores de *zfs* indican, sin
embargo, que esto no es para indicar que *zfs* sea capaz de
direccionar hasta zettabytes de información, sino que será /el último
sistema de archivos/ que cualquier administrador requerirá.] fue
desarrollado por Sun Microsystems desde el año 2001, forma parte del
sistema operativo /Solaris/ desde el 2005, y hoy en día puede
emplearse desde los principales sistemas operativos libres.[fn:: *zfs*
no puede ser incorporado íntegramente al núcleo de Linux por
/incompatibilidad de licencias/: si bien ambos son software libre, los
modelos de licenciamiento *gpl* (de Linux) y *cddl* (de *zfs*) son
incompatibles.] Y si bien *zfs* resulta suficientemente atractivo tan
sólo por haber sido diseñado para que el usuario nunca más se tope con
un límite impuesto por el sistema operativo, el principal cambio que
presenta al usuario es una forma completamente distinta de referirse
al almacenamiento.

En primer término, al igual que *lvm* presenta una primer integración
entre conceptos, permitiendo unir de diferentes maneras varios
dispositivos físicos en un dispositivo lógico, *zfs* incluye en la misma
lógica administrativa al sistema de archivos: en la configuración
estándar, basta conectar una unidad al sistema para que ésta aparezca
como espacio adicional disponible para los usuarios. El espacio
combinado de todas las unidades conforma un /fondo de almacenamiento/
(/storage pool/).

La lógica de *zfs* parte de que operará una /colección/ de sistemas de
archivos en una organización jerárquica. Pero a diferencia del esquema
tradicional Unix en que cada sistema de archivos es preparado desde un
principio para su función, en *zfs* se pueden aplicar límites a
jerarquías completas. Bajo un esquema *zfs*, la creación y el montaje de
un sistema de archivos es una operación sencilla — al grado que se
presenta como recomendación que, para cada usuario en el sistema, se
genere un sistema de archivos nuevo e independiente.

Una de las principales diferencias con los sistemas de archivos
tradicionales es el manejo del espacio vacío: el espacio disponible
total del fondo de almacenamiento se reporta como disponible /para
todos los sistemas de archivos/ que formen parte de éste. Sin embargo,
se pueden indicar /reservas/ (mantener un mínimo del espacio
especificado disponible para determinado subconjunto de sistemas de
archivos dentro de la colección) y /límites/ (evitar que el uso de una
colección exceda el almacenamiento indicado) para las necesidades de
las diferentes regiones del sistema.

* Ejercicios

** Preguntas de autoevaluación

1. Se presentaron algunos algoritmos para gestionar las solicitudes de
   acceso a disco — /Primero llegado, primero servido/ (*fifo*), /Tiempo
   más corto a continuación/ (*sstf*), /Elevador/ (*scan*), y algunas de
   sus variaciones. Se mencionó también que, a pesar de la importancia
   de conocerlos por su importancia histórica, hoy en día han dejado
   de ser tan importantes como lo fueron hacia los 1980. Mencione dos
   factores que han llevado a que pierdan relevancia.

2. Realice un esquema de cómo se estructura cada bloque de información
   sobre varios discos bajo *raid* niveles 0, 1 y 5. Para cada uno de
   estos niveles, indique el efecto que su empleo tendría en cuanto a
   espacio total, velocidad de acceso y confiabilidad.


** Temas de investigación sugeridos

- Detalles de los sistemas de archivos en Flash ::

  En este apéndice se expusieron los principales puntos de los medios
  de /estado sólido/ o /no rotativos/, apuntando apenas hacia cómo
  podrían estos aprovecharse mejor.

  ¿Qué sistemas de archivos están mejor /afinados/ para operar con
  medios Flash, y cuáles son los principales obstáculos para que gocen
  de una mayor adopción?

** Lecturas relacionadas
- \fullcite{Constantin2010}
- \fullcite{FreeBSDZFS}
- \fullcite{CorbetHashDoSBtrfs}
- \fullcite{Fitzpatrick2011}
- \fullcite{Rosenblum1991}
- \fullcite{Engel2005}
- \fullcite{Aurora2009}
- \fullcite{Brown2012},
- \fullcite{Patterson1988}
- \fullcite{Cano2013}
- \fullcite{Guo2013}
- \fullcite{Cogent2013}
