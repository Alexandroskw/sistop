#+TITLE: Sistemas Operativos — Administración de procesos
#+AUTHOR: Gunnar Wolf
#+EMAIL: gwolf@gwolf.org
#+LANGUAGE: es
#+INFOJS_OPT: tdepth:1 sdepth:1 ftoc:nil ltoc:nil

* Procesos. Concepto y estados de un proceso

En un sistema multiprogramado o de tiempo compartido, un /proceso/ es
la imagen en memoria de un programa, junto con la información
relacionada con el estado de su ejecución.

Un programa es una /entidad pasiva/, una lista de instrucciones; un
proceso es una /entidad activa/, que –empleando al programa– define la
actuación que tendrá el sistema.

En contraposición con /proceso/, hablaríamos de /tareas/ en un sistema
por lotes. Una tarea requiere mucha menos información, típicamente
bastaría con guardar información relacionada con la /contabilidad/ de
los recursos empleados. Una tarea no es interrupmida en el transcurso
de su ejecución. Ahora bien, esta distinción no es completamente
objetiva — Y encontraremos muchos textos que emplean indistintamente
una u otra nomenclatura.

Pero si bien el sistema nos da la /ilusión/ de que muchos procesos se
están ejecutando al mismo tiempo, la mayor parte de ellos típicamente
están esperando estar listos para su ejecución — en un momento
determinado sólo puede estarse ejecutando un número de procesos igual
o menor al número de CPUs que tenga el sistema.

En esta sección del curso nos ocuparemos de los conceptos relacionados
con procesos, hilos, concurrencia y sincronización — Abordaremos las
técnicas y algoritmos que emplea el sistema operativo para determinar
cómo y en qué órden hacer los cambios de proceso que nos brindan la
ilusión de simultaneidad en la sección /[[./planificacion_de_procesos.org][Planificación de procesos]]/.

** Estados de un proceso

Un proceso, a lo largo de su vida, alterna entre diferentes /estados/
de ejecución. Estos son:

- Nuevo :: Se solicitó al sistema operativo la creación de un proceso,
           y sus recursos y estructuras están siendo creadas

- Listo :: Está listo para ser asignado para su ejecución en un procesador

- En ejecución :: El proceso está siendo ejecutado en este momento

- Bloqueado :: En espera de algún evento para poder continuar
               ejecutándose

- Terminado :: El proceso terminó de ejecutarse; sus estructuras están
               a la espera de ser /limpiadas/ por el sistema operativo

#+begin_center
#+attr_html: height="350"
#+attr_latex: width=0.5\textwidth
[[./img/estados_proceso.png]]

Diagrama de transición entre los estados de un proceso
#+end_center

** El bloque de control de proceso (PCB)

La información que debe manipular el sistema operativo relativa a cada
uno de los procesos en ejecución (sea cual sea su estado) se compone
de:

- Estado del proceso :: El estado actual del proceso

- Contador de programa :: Cuál es la siguiente instrucción a ser
     ejecutada por el proceso.

- Registros del CPU :: La información específica del estado del CPU
     mientras el proceso está en ejecución debe ser respaldada y
     restaurada cuando se registra un cambio de estado

- Información de agendamiento (scheduling) :: La prioridad del
     proceso, la /cola/ en que está agendado, y demás información que
     puede ayudar al sistema operativo a agendar al proceso;
     profundizaremos en el tema en la sección /[[./planificacion_de_procesos.org][Planificación de
     procesos]]/.

- Información de administración de memoria :: Las tablas de mapeo de
     memoria (páginas o segmentos, dependiendo del sistema
     operativo). Abordaremos el tema en la sección /[[./administracion_de_memoria.org][Administración de
     memoria]]/.

- Información de contabilidad :: Información de la utilización de
     recursos que ha tenido este proceso — Puede incluir el tiempo
     total empleado (/de usuario/, cuando el CPU va avanzando sobre
     las instrucciones del programa propiamente, /de sistema/ cuando
     el sistema operativo está atendiendo las solicitudes realizadas
     por él), uso acumulado de memoria y dispositivos, etc.

- Estado de E/S :: Listado de dispositivos y archivos asignados que el
                   proceso tiene /abiertos/ en un momento dado.

* Procesos e hilos

Como vimos, la cantidad de información que el sistema operativo debe
manejar acerca de cada proceso es bastante significativa. Si cada vez
que el /planificador/ elige qué proceso pasar de /Listo/ a /En ejecución/
debe considerar buena parte de dicha información, la simple
transferencia de todo esto entre la memoria y el CPU podría llevar a
un desperdicio /burocrático/ de recursos. Una respuesta a esta
problemática fue la de los /hilos de ejecución/, a veces conocidos
como /procesos ligeros/ (Lightweight processes, LWP).

Cuando consideramos procesos basados en un modelo de hilos, podríamos
proyectar en sentido inverso que todo proceso es como un sólo hilo de
ejecución. Un sistema operativo que no ofreciera soporte expreso a los
hilos lo agendaría exactamente del mismo modo.

Pero visto desde la perspectiva del proceso hay una gran diferencia:
Si bien el sistema operativo se encarga de que cada proceso tenga una
visión de virtual exclusividad sobre la computadora, todos los hilos
de un proceso comparten un sólo espacio de direccionamiento en memoria
y lista de descriptores de archivos y dispositivos abiertos. Cada uno
de los hilos se ejecuta de forma (aparentemente) secuencial y maneja
su propio contador de programa (y algunas estructuras adicionales,
aunque mucho más ligeras que el PCB).

** Los hilos y el sistema operativo

Formalmente, una programación basada en hilos puede hacerse
completamente y de forma transparente en espacio de usuario (sin
involucrar al sistema operativo). Estos hilos se llaman /hilos de
usuario/ (/user threads/), y muchos lenguajes de programación los
denominan /hilos verdes/ (/green threads/). Un caso de uso interesante
es en sistemas operativos mínimos (p.ej. para dispositivos embebidos)
capaces de ejecutar una máquina virtual de alguno de estos lenguajes:
Si bien el sistema operativo no maneja multiprocesamiento, a través de
los hilos de usuario sí podemos crear procesos con multitarea interna.

Los procesos que implementan hilos ganan un poco en el rendimiento,
pero mucho para compartir espacio de memoria sin tenerlo que
establecer explícitamente a través de mecanismos de comunicación entre
procesos. Muchas veces (dependiendo de la plataforma) los hilos de
usuario utilizan multitarea cooperativa para pasar el control de un
hilo a otro. Cualquier llamada al sistema /bloqueante/ (como obtener
datos de un archivo para utilizarlos inmediatamente) interrumpirá la
ejecución de todos los hilos, dado que el control de ejecución es
entregado al sistema operativo.

El siguiente paso fue la creación de hilos /informando/ al sistema
operativo, típicamente denominados /hilos de kernel/ (/kernel
threads/). A través de bibliotecas de sistema que los implementan de
forma estándar para los diferentes sistemas operativos
(p.ej. =pthreads= para POSIX o =Win32_Thread= para Windows) o
arquitecturas (/hilos verdes/, en Java; Perl tiene un
modelo propio, modelado de cerca al estilo de POSIX). Estas
bibliotecas aprovechan la comunicación con el sistema operativo tanto
para solicitudes de recursos (p.ej. un proceso basado en hilos puede
beneficiarse de una ejecución verdaderamente paralela en sistemas
multiprocesador) como para una gestión de recursos más comparable con
una situación de multiproceso estándar.

** Patrones de trabajo con hilos

Hay tres patrones en los que caen generalmente los modelos de hilos;
podemos emplear a más de uno de estos patrones en diferentes áreas de
nuestra aplicación:

- Jefe / trabajador :: Un hilo tiene una tarea distinta de todos los
     demás: El hilo /jefe/ genera o recopila tareas que requieren ser
     cubiertas, las separa y se las entrega a los hilos
     /trabajadores/.

     Este modelo es el más común para procesos que implementan
     servidores y para aplicaciones gráficas (GUIs), en que hay una
     porción del programa (el hilo /jefe/) esperando a que ocurran
     eventos externos. El jefe realiza poco trabajo, aunque puede
     llevar contabilidad de los trabajos realizados.

- Equipo de trabajo :: Al iniciar la porción multihilos del proceso,
     se crean muchos hilos idénticos, que realizarán las mismas tareas
     sobre diferentes datos. Este modelo es muy frecuentemente
     utilizado para cálculos matemáticos (p.ej. criptografía,
     render). Puede combinarse con un estilo jefe/trabajador para irle
     dando al usuario una previsualización del resultado de su
     cálculo, dado que éste se irá ensamblando progresivamente, pedazo
     por pedazo.

- Línea de ensamblado :: Si una tarea larga puede dividirse en pasos
     sobre bloques de la información total a procesar, cada hilo puede
     enfocarse a hacer sólo una tarea y pasarle los datos a otro hilo
     conforme vaya terminando. Una de las principales ventajas de este
     modelo es que nos ayuda a mantener rutinas simples de comprender,
     y permite que el procesamiento de datos continúe incluso si parte
     del programa está bloqueado esperando E/S.

* Concurrencia

#+begin_center
#+attr_html: max-width: 80%;
Para el estudio de este tema, recomiendo fuertemente referirse al
libro «[[Little_Book_of_Semaphores_-_Allen_Downey.pdf][The little book of semaphores]]» de Allen Downey (2008).

Pueden descargar (legalmente) el libro desde el sitio Web del curso o
desde [[http://www.greenteapress.com/semaphores/index.html][Green Tea Press]].
#+end_center

Formalmente y desde las ciencias de la computación, /concurrencia/ no
necesariamente se refiere a dos o más eventos que ocurran a la vez,
sino que a dos o más eventos cuyo órden es /no determinista/, esto es,
eventos acerca de los cuales /no podemos predecir el órden relativo en
que ocurrirán/. Esto puede ocurrir porque hablamos de dos hilos
ejecutándose en conjunto, dos procesos independientes en el mismo
equipo, o incluso procesos independientes en computadoras separadas
geográficamente; el estudio de situaciones derivadas de la
concurrencia es uno de los campos de estudio clásico (y más abstracto)
de las ciencias de la computación.

Si bien una de las tareas principales de los sistemas operativos es
dar a cada proceso la ilusión de que se está ejecutando en una
computadora dedicada, de modo que el programador no tenga que pensar
en la competencia por recursos, a veces esta ilusión sencillamente no
puede presentarse — Parte del desarrollo de un programa puede depender
de datos obtenidos en fuentes externas a éste, y la cooperación con
hilos o procesos externos es fundamental.

Para algunos de los ejemplos a continuación, presentaremos ejemplos
usando la semántica de la interacción entre hilos del mismo proceso,
sincronización entre procesos independientes, asignación de recursos
por parte del núcleo a procesos simultáneos, o incluso entre usuarios
de diferentes equipos de una red — En todos estos casos, los conceptos
presentados pueden generalizarse a los demás, y son situaciones en que
se presenta compartición (o competencia) por estructuras entre
entes independientes.

** Exclusión mutua y sincronización

Varios hilos pueden avanzar en su trabajo de forma concurrente sin
entorpecerse mutuamente siempre y cuando estén trabajando únicamente
con /variables locales/, esto es, valores independientes para cada uno
de los hilos. Sin embargo, cuando dos hilos tienen que
/sincronizarse/ (asegurar un ordenamiento dado entre flujos
independientes de ejecución), o cuando tienen que transmitirse
información, el uso de /variables globales/ y de recursos externos
requiere tener en mente que el planificador puede interrumpir el flujo de
un hilo /en cualquier momento/. Esto implica, por ejemplo, que el
siguiente código en Ruby puede llevarnos a distintos resultados:

#+begin_src ruby -n
class EjemploHilos
  def initialize
    @x = 0
  end

  def f1
    sleep 0.1
    @x += 3
  end

  def f2
    sleep 0.1
    @x *= 2
  end

  def run
    t1 = Thread.new {f1}
    t2 = Thread.new {f2}
    sleep 0.1
    print @x + ' '
  end
end
#+end_src

En este ejemplo, inserté un tiempo de espera largo, de una décima de
segundo (=sleep 0.1=) para obligar al planificador a elegir a alguno de
los hilos tras un periodo de espera (en caso contraio, las funciones
son tan simples que, bajo la implementación de Ruby, se ejecutaría
simplemente en forma secuencial.

La /variable de instancia/ =@x= es compartida entre los dos hilos de
ejecución, y en este ejemplo tenemos tres hilos /compitiendo/ por
ella. En algunas ejecuciones, =run= ejecutará primero la
multiplicación, resultando en =(@x * 2) + 3=, en otras =(@x + 3) * 2=
(siendo hilos diferentes, no vale la precedencia de los
operadores). Algunas veces imprimirá el resultado antes de ambas
operaciones (el =@x= original, en el estado de entrada de los hilos),
en otros a medio camino, y en otras más después de ambas
modificaciones. Es más, a veces el valor resultante de =@x= puede
/aparentar que una de las operaciones no ocurrió, dado que un hilo fue
interrumpido a media operación:

#+begin_src ruby
e = EjemploHilos.new;10.times{e.run}
6 9 21 45 180 183 372 750 1500 3006

e = EjemploHilos.new;10.times{e.run}
0 3 15 33 66 135 276 1110 1110 2226
#+end_src

Y si bien este pequeño programa fue hecho explícitamente para ilustrar
este problema, en un programa real con hilos de ejecución complejos,
el no saber dónde será interrumpido el flujo presenta un problema
mayor: ¿cómo pueden dos hilos manipular un recurso compartido si no
hay garantía de que una operación no será interrumpida? Y recordemos
que las instrucciones que le damos al sistema no tienen por qué
traducirse a una sóla instrucción ante el sistema — Una instrucción en
C tan simple como =x++= implica por lo menos:

- Obtener la dirección en memoria de =x=
- Traer el valor de =x= a un registro del procesador
- Incrementar ese valor en 2
- Almacenar el valor del registro en la memoria

Al haber dos accesos a memoria (¡y estamos hablando de un lenguaje de
mucho más bajo nivel que el del ejemplo!), el CPU puede tener que
esperar a que el valor le sea transferido, y al planificador puede
aprovechar para cambiar el hilo en ejecución. Claro está, con un
lenguaje de tan alto nivel como Ruby, el número de instrucciones
resultante puede ser mucho mayor.

- Operación atómica :: Operación que tenemos la garantía que se
     ejecutará o no como una sóla unidad de ejecución. Esto no
     necesariamente implica que el sistema no retirará el flujo de
     ejecución de su hilo, sino que /el efecto de que se le retire el
     flujo/ no llevará a comportamiento inconsistente.

- Condición de carrera :: (Race condition) Categoría de errores de
     programación que implica a dos procesos fallando al comunicarse
     su estado mutuo, llevando a resultados inconsistentes. Es uno de
     los problemas más frecuentes y difíciles de depurar, y ocurre
     típicamente por no considerar la /no atomicidad/ de una operación

- Sección crítica :: El área de código que requiere ser protegida de
     accesos simultáneos, donde se realiza la modificiación de datos
     compartidos.

Dado que el sistema no tiene forma de saber cuáles instrucciones (o
áreas del código) requerimos que funcionen de forma atómica, nosotros
debemos indicárselo de forma explícita, sincronizando nuestros hilos
(o procesos). Es necesario asegurarnos que la sección crítica no
permitirá la entrada de dos hilos de forma casi-simultánea.

Un error muy común es utilizar mecanismos /no atómicos/ para señalizar
al respecto. Consideremos que estamos haciendo un sistema de venta de
boletos de autobús en Perl, y queremos hacer la siguiente función
/segura ante la concurrencia/. El programador aquí ya hizo un primer
intento:

#+begin_src perl -n
my ($proximo_asiento :shared, $capacidad :shared, $bloq :shared);
$capacidad = 40;

sub asigna_asiento {
  while ($bloq) { sleep 0.1; }
  $bloq = 1;
  if ($proximo_asiento < $capacidad) {
    $asignado = $proximo_asiento;
    $proximo_asiento += 1;
    print "Asiento asignado: $asignado\n";
  } else {
    print "No hay asientos disponibles\n";
    return 1;
  }
  $bloq = 0;
  return 0;
}
#+end_src

El programador identificó correctamente la /sección crítica/ como las
líneas comprendidas entre la 7 y la 9 (pero, al ser parte de un bloque
condicional, /protegió/ hasta la 14). Sin embargo, tenemos aún una
situación de carrera (aunque mucho más contenida) entre la 2 y la 3:
Podría un hilo entrar[fn:: Este ejemplo utiliza además el mal ejemplo
de una /espera activa/ (busy wait), requiriendo del tiempo del
procesador periódicamente mientras espera a que se satisfaga una
condición dada. Veremos cómo evitar esto más adelante.] al =while= y
evaluar a un =$bloq= aún falso, y –justo antes de modificarlo– el
control se transfiere a otro hilo entrando al mismo lugar, y vendiendo
dos veces el mismo asiento.

Para señalizar la entrada a una sección crítica no podemos hacerlo
desde el flujo susceptible a ser interrumpido, tenemos que hacerlo a
través de instrucciones de las que el planificador pueda /asegurar/ su
atomicidad.

*** /Mutexes/

La palabra /mutex/ nace de la frecuencia con que se habla de las
/regiones de exclusión mutua/ (en inglés, /mutual exclusion/). Es un
mecanismo que nos asegura que cierta región del código será ejecutada
como si fuera atómica.

Hay que tener en cuenta que un mutex /no significa/ que el código no
se va a interrumpir mientras está dentro de esta región — Eso sería
muy peligroso, dado que permitiría que el sistema operativo perdiera
el control del planificador, volviendo para propósitos prácticos a un
esquema de multitarea cooperativa. El mutex es un /mecanismo de
prevención/ que mantiene en espera a cualquier hilo o proceso que
quiera entrar a la /sección crítica/ hasta que el proceso que la está
ejecutando en un momento dado salga de ella.

Como vimos en el ejemplo anterior, para que una mutex sea efectiva
tiene que ser implementada a través de una /primitiva/ a un nivel
superior, implicando al planificador.

El código del ejemplo anterior podría reescribirse de la siguiente
manera empleando un mutex:

#+begin_src perl -n
my ($proximo_asiento :shared, $capacidad :shared);
$capacidad = 40;

sub asigna_asiento {
  lock($proximo_asiento);
  if ($proximo_asiento < $capacidad) {
    $asignado = $proximo_asiento;
    $proximo_asiento += 1;
    print "Asiento asignado: $asignado\n";
  } else {
    print "No hay asientos disponibles\n";
    return 1;
  }
  return 0;
}
#+end_src

Tomemos en cuenta que en este caso estamos hablando de una
implementación de hilos — Y como lo mencionamos previamente, esto nos
hace dependientes del lenguaje específico de implementación. En este
caso, en Perl, al ser =proximo_asiento= una variable compartida tiene
algunas /propiedades/ adicionales — Como, en este caso, la de poder
operar como un mutex. La implementación en Perl resulta muy /limpia/,
dado que nos evita el uso de una /variable de condición/ explícita —
Podríamos leer la línea 5 como /exclusión mutua sobre/
=$proximo_asiento=.

En la implementación de hilos de Perl, la función =lock()= implementa
un mutex delimitado por el /ámbito léxico/ de su invocación: El área
de exclusión mutua abarca desde la línea 5 en que es invocada hasta la
15 en que termina el bloque en que se invocó.

Un área de exclusion mutua debe:

- Ser mínima :: Debe ser /tan corta como sea posible/, para evitar que
                otros hilos queden bloqueados fuera del área
                crítica. Si bien en este ejemplo es demasiado simple,
                si hiciéramos cualquier llamada a otra función (o al
                sistema) estando dentro de un área de exclusión mutua,
                detendríamos la ejecución de todos los demás hilos por
                demasiado tiempo.

- Ser comprehensiva :: Debemos analizar bien cuál es el área a
     proteger y no arriesgarnos a proteger de menos. En este ejemplo,
     podríamos haber puesto =lock($asignado)= dentro del =if=, dado
     que sólo dentro de su evaluación positiva modificamos la variable
     =$proximo_asiento=. Sin embargo, si la ejecución de un hilo se
     interrumpiera entre las líneas 7 y 8, la condición del =if= se
     evaluaría incorrectamente.

Como comparación, una rutina equivalente en Bash (entre procesos
independientes y usando los archivos =/tmp/proximo_asiento= y
=/etc/capacidad/= como un mecanismo para compartir datos) sería:

#+begin_src bash -n
asigna_asiento() {
  lockfile /tmp/asigna_asiento.lock
  PROX=$(cat /tmp/proximo_asiento || echo 0)
  CAP=$(cat /etc/capacidad || echo 40)
  if [ $PROX -lt $CAP ]
    then
      ASIG=$PROX
      echo $(($PROX+1)) > /tmp/proximo_asiento
      echo "Asiento asignado: $ASIG"
    else
      echo "No hay asientos disponibles"
      return 1;
    fi
  rm -f /tmp/asigna_asiento.lock
}
#+end_src

Un mutex es, pues, una herramienta muy sencilla, y podría verse como
la pieza básica para la sincronización entre procesos. Lo fundamental
para emplearlos es identificar las regiones críticas de nuestro
código, y proteger el acceso /con un mecanismo apto de
sincronización/, que garantice atomicidad.

*** Semáforos

La interfaz ofrecida por los mutexes es muy sencilla, pero no permite
resolver algunos problemas de sincronización. Edsger Dijkstra (1968) propuso
a los /semáforos/.

Un semáforo es una variable de tipo entero que tiene definida la
siguiente interfaz:

- Inicialización :: Se puede inicializar el semáforo a cualquier valor
                    entero, pero después de esto, su valor no puede ya
                    ser leído.

- Decrementar :: Cuando un hilo decrementa el semáforo, si el valor es
                 negativo, el hilo se /bloquea/ y no puede continuar
                 hasta que /otro hilo/ incremente el semáforo

- Incrementar :: Cuando un hilo incrementa al semáforo, si hay hilos
                 epserando, uno de ellos es /despertado/.

Las operaciones de decrementar e incrementar muchas veces son
implementadas como =wait= y =signal= (siguiendo la semántica de los
semáforos de tren). En ciertos textos los veremos referidos también
como =P= y =V=, los nombres empleados por Dijkstra en su artículo (de
/proberen/ y /verhogen/, en holandés), =down= y =up=, o =acquire= y
=release=.

Un semáforo permite la implementación de varios patrones:

- Señalizar :: Un hilo debe informar a otro que cierta condición está
               ya cumplida — Por ejemplo, un hilo prepara una conexión
               en red mientras que otro calcula lo que tiene que
               enviar. No podemos arriesgarnos a comenzar a enviar
               antes de que la conexión esté lista. Inicializamos el
               semáforo a 0, y:

	       #+begin_src python -n
	       # Antes de lanzar los hilos
	       senal = Semaphore(0)

	       def envia_datos:
	         calcula_datos()
		 senal.acquire()
		 envia_por_red()

	       def prepara_conexion:
	         crea_conexion()
		 senal.release()

	       #+end_src

	       No importa si =prepara_conexion()= termina primero — En
	       el momento en que termine, =senal= valdrá 1 y
	       =envia_datos()= podrá proceder.

- /Rendezvous/ :: Así se denomina en francés (y ha sido adoptado al
                  inglés) a quedar en una /cita/. Este patrón busca
                  que dos hilos se esperen mutuamente en cierto punto
                  para continuar en conjunto — Por ejemplo, en una
                  aplicación GUI, un hilo prepara la interfaz gráfica
                  y actualiza sus eventos mientras otro efectúa
                  cálculos para mostrar. Queremos mostrar al usuario
                  la simulación desde el principio, así que no debe
                  empezar a calcular antes de que el GUI esté listo,
                  pero preparar los datos del cálculo toma tiempo, y
                  no queremos esperar doblemente. Para esto,
                  implementamos dos semáforos señalizándose
                  mutuamente:

		  #+begin_src python -n
		  guiListo = Semaphore(0)
		  calculoListo = Semaphore(0)

		  threading.Thread(target=maneja_gui, args=[]).start()
		  threading.Thread(target=maneja_calculo, args=[]).start()

		  def maneja_gui():
		    inicializa_gui()
		    guiListo.release()
		    calculoListo.acquire()
		    recibe_eventos()

		  def maneja_calculo():
		    inicializa_datos()
		    calculoListo.release()
		    guiListo.acquire()
		    procesa_calculo()
		  #+end_src

- Torniquete :: Una construcción que por sí sóla no hace mucho, pero
                resulta útil para paatrones posteriores. 

- Mutex :: El uso de un semáforo inicializado a 1 puede implementar
	   fácilmente un mutex. En Python:

           #+begin_src python -n
	   mutex = Semaphore(1)
	   # ...Inicializamos estado y lanzamos hilos
	   mutex.acquire()
	   # Estamos en la región de exclusión mutua
	   x = x + 1
	   mutex.release()
	   # Continúa la ejecución paralela
           #+end_src

- Multiplex :: Permite la entrada de no más de /n/ procesos a la
               región crítica. Si lo vemos como una generalización de
               /Mutex/, basta con inicializar al semáforo al número
               máximo de procesos deseado.

- Barrera ::

- Barrera reutilizable ::

- Cola ::

- Cola FIFO ::

* Bloqueos mutuos

- Bloqueo mutuo :: (o /interbloqueo/; en inglés, /deadlock/) Situación
                   que ocurre cuando dos procesos poseen determinados
                   recursos, y cada uno queda detenido, a la espera de
                   alguno de los que tiene el otro. El sistema puede
                   seguir operando normalmente, pero los procesos en
                   cuestión no podrán avanzar.

- Inanición :: (en inglés /resource starvation/): Situación en que un
	       proceso no es agendado para su ejecución dado que los
	       recursos por los cuales está esperando son asignados a
	       otros procesos.

Un bloqueo mutuo puede ejemplificarse con la situación que se presenta
cuando cuatro automovilistas llegan al mismo tiempo al cruce de dos
avenidas del mismo rango en que no hay un semáforo, cada uno desde
otra dirección. Los reglamentos de tránsito señalan que la precedencia
la tiene /el automovilista que viene más por la derecha/. En este
caso, cada uno de los cuatro debe ceder el paso al que tiene a la
derecha — Y ante la ausencia de un criterio humano que rompa el
bloqueo, deberían todos mantenerse esperando por siempre.

Un bloqueo mutuo se presenta cuando (/Condiciones de Coffman/) (La
Red, p. 185)

1. Los procesos reclaman control exclusivo de los recursos que piden
   (condición de /exclusión mutua/).

2. Los procesos mantienen los recursos que ya les han sido asignados
   mientras esperan por recursos adicionales (condición de /espera
   por/).

3. Los recursos no pueden ser extraídos de los procesos que los tienen
   hasta su completa utilización (condición de /no apropiatividad/).

4. Existe una cadena circular de procesos en la que cada uno mantiene a
   uno o más recursos que son requeridos por el siguiente proceso de la
   cadena (condición de /espera circular/).

Las primeras tres condiciones son /necesarias pero no suficientes/
para que se produzcaun bloqueo; su presencia puede llamar nuestra
atención hacia una situación de riesgo. Sólo cuando se presentan las
cuatro podemos hablar de un bloqueo mutuo efectivo.

Otro ejemplo clásico es un sistema con dos unidades de cinta
(dispositivos de acceso secuencial y no compartible), en que los
procesos /A/ y /B/ requieren de ambas unidades. Supongamos siguiente
secuencia:

1. /A/ solicita una unidad de cinta y se bloquea

2. /B/ solicita una unidad de cinta y se bloquea

3. El sistema operativo otorga la unidad /1/ a /A/.y lo vuelve a poner
   en ejecución

4. /A/ continúa procesando; termina su periodo de ejecución

5. El sistema operativo otorga la unidad /2/ a /B/ y lo vuelve a poner
   en ejecución

6. /B/ solicita otra unidad de cinta y se bloquea

7. El sistema operativo no tiene otra unidad de cinta por
   asignar. Mantiene a /B/ bloqueado; otorga el control de vuelta a
   /A/

8. /A/ solicita otra unidad de cinta y se bloquea

9. El sistema operativo no tiene otra unidad de cinta por
   asignar. Mantiene a /B/ bloqueado; otorga el control de vuelta a
   otro proceso (o queda en espera)

#+begin_center
#+attr_html: height="350"
#+attr_latex: width=0.5\textwidth
[[./img/bloqueo_mutuo_simple.png]]

Esquema clásico de un bloqueo mutuo simple: Los procesos /A/ y /B/
esperan mutuamente para el acceso a las unidades de cinta /1/ y /2/.
#+end_center

Sin una política de prevención o resolución de bloqueos mutuos, no hay
modo de que /A/ o /B/ continúen su ejecución.

** Estrategias ante los bloqueos

En el apartado de /Exclusión mutua/, los hilos presentados estaban
diseñados para /cooperar explícitamente/. El rol del sistema operativo
va más allá, tiene que implementar /políticas/ que eviten, en la
medida de lo posible, dichos bloqueos.

Las políticas tendientes a otorgar los recursos lo antes posible
cuando son solicitadas pueden ser vistas como /liberales/, en tanto
que las que controlan más la asignación de recursos,
/conservadoras/.

#+begin_center
#+attr_html: height="200"
#+attr_latex: width=0.9\textwidth
[[./img/deadlocks_conserv_lib.png]]

Espectro liberal—conservador de esquemas para evitar bloqueos
(Finkel, 1988, p.128)
#+end_center

Las líneas principales que describen a las estrategias para enfrentar
situaciones de bloqueo (La Red, p. 188):

- Prevención :: Se centra en modelar el comportamiento del sistema
		para que /elimine toda posibilidad/ de que se produzca
		un bloqueo. Resulta en una utilización subóptima de
		recursos.

- Evasión :: Busca imponer condiciones menos estrictas que en la
	     prevención, para intentar lograr una mejor utilización de
	     los recursos. Si bien no puede evitar /todas las
	     posibilidades/ de un bloqueo, cuando éste se produce
	     busca /evitar/ sus consecuencias.

- Detección y recuperación :: El sistema /permite/ que ocurran los
     bloqueos, pero busca /determinar si ha ocurrido/ y tomar medidas
     para eliminarlo.

     Busca despejar los bloqueos presentados para que el sistema
     continúe operando sin ellos.

** Prevención de bloqueos

Una manera de evitar bloqueos /por completo/ sería el que un sistema
operativo jamás asignara recursos a más de un proceso a la vez — Los
procesos podrían seguir efectuando cálculos o empleando recursos /no
rivales/ (que no requieran acceso exclusivo — Por ejemplo, empleo de
archivos en el disco, sin que exista un acceso directo del proceso al
disco), pero sólo uno podría obtener recursos de forma exclusiva al
mismo tiempo. Este mecanismo sería la /serialización/, y la situación
antes descrita se resolvería de la siguiente manera:

1. /A/ solicita una unidad de cinta y se bloquea

2. /B/ solicita una unidad de cinta y se bloquea

3. El sistema operativo otorga la unidad /1/ a /A/ y lo vuelve a poner
   en ejecución

4. /A/ continúa procesando; termina su periodo de ejecución

5. El sistema operativo mantiene bloqueado a /B/, dado que /A/ tiene
   un recurso

6. /A/ solicita otra unidad de cinta y se bloquea

7. El sistema operativo otorga la unidad /2/ a /A/ y lo vuelve a poner
   en ejecución

8. /A/ libera la unidad de cinta /1/

9. /A/ libera la unidad de cinta /2/ (y con ello, el bloqueo de uso de
   recursos)

10. El sistema operativo otorga la unidad /1/ a /B/ y lo vuelve a
    poner en ejecución

11. /B/ solicita otra unidad de cinta y se bloquea

12. El sistema operativo otorga la unidad /2/ a /B/ y lo vuelve a
    poner en ejecución

13. /B/ libera la unidad de cinta /1/

14. /B/ libera la unidad de cinta /2/

Si bien la serialización resuelve la situación aquí mencionada, el
mecanismo empleado es subóptimo dado que puede haber hasta /n-1/
procesos esperando a que uno libere los recursos.

Un sistema que implementa una política de asignación de recursos
basada en la serialización, si bien no caerá en bloqueos mutuos, sí
tiene un peligro fuerte de caer en /inanición/.

Otro ejemplo de política preventiva /menos conservadora/ sería la
/retención y espera/ o /reserva/ (/advance claim/): Que todos los
programas declaren al iniciar su ejecución qué recursos van a
requerir. Los recursos son apartados para su uso exclusivo hasta que
el proceso termina, pero el sistema operativo puede seguir atendiendo
solicitudes /que no rivalicen/: Si a los procesos /A/ y /B/ anteriores
se suman procesos /C/ y /D/, pero requieren otro tipo de recursos,
podrían ejecutarse en paralelo /A/, /C/ y /D/, y una vez que /A/
termine, podrían continuar ejecutando /B/, /C/ y /D/.

El bloqueo resulta ahora imposible por diseño, pero el usuario que
inició /B/ tiene una percepción de injusticia dado el tiempo que tuvo
que esperar para que su solicitud fuera atendida — De hecho, si /A/ es
un proceso de larga duración (incluso si requiere la unidad de cinta
sólo por un breve periodo), esto lleva a que /B/ sufra una /inanición/
innecesariamente prolongada.

Además, la implementación de este mecanismo preventivo requiere que el
programador sepa por anticipado qué recursos requerirá — Y esto en la
realidad muchas veces es imposible. Si bien podría diseñarse una
estrategia de lanzar procesos /representantes/ (o /proxy/) solicitando
recursos específicos cuando éstos hicieran falta, esto sólo
transferiría la situación de bloqueo por recursos a bloqueo por
procesos — y un programador poco cuidadoso podría de todos modos
desencadenar la misma situación.

** Evasión de bloqueos

Para la evasión de bloqueos, el sistema partiría de poseer, además de
la información descrita en el caso anterior, información acerca de
/cuándo/ requiere un proceso utilizar cada recurso. De este modo,
el planificador puede marcar qué flujos entre dos (o más) procesos son
/seguros/ y cuáles son /inseguros/

#+begin_center
#+attr_html: height="350"
#+attr_latex: width=0.8\textwidth
[[./img/tray_proc_evasion_bloqueo.png]]

Evasión de bloqueos: Los procesos /A/ (horizontal) y /B/ (vertical)
requieren del acceso exclusivo a un scanner y una impresora. (La Red,
p. 200)
#+end_center

El análisis de la interacción entre dos procesos se representa como en
la figura anterior; el avance en cada proceso es marcado con una
flecha horizontal (/A/) o vertical (/B/); en un sistema
multiprocesador, podría haber avance mutuo, y lo indicaríamos con una
flecha diagonal.

Al saber cuándo reclama y libera un recurso cada proceso, podemos
marcar cuál es el área /segura/ para la ejecución y cuándo estamos
aproximándonos a un área de riesgo. En el caso mostrado, el bloqueo
mutuo se produciría si entráramos a I_2—I_3 e I_6—I_7, por lo que –en
la situación descrita en esta gráfica– el sistema debe mantener a /B/
congelado por lo menos hasta que /A/ llegue a I_3.

Este mecanismo proveería una mejor respuesta que los vistos en el
apartado de /prevención de bloqueos/, pero es todavía más dificil de
aplicar en situaciones reales. Para que pudiéramos implementar un
sistema con evasión de bloqueos, tendría que ser posible hacer un
análisis estático previo del código a ejecutar, y tener un listado
total de recursos estático. Estos mecanismos pueden ser efectivos en
sistemas de uso especializado, pero no en sistemas operativos (o
planificadores) genéricos.

** Detección y recuperación de bloqueos

La detección de bloqueos es una forma de /reaccionar/ ante una
situación de bloqueo que ya se presentó y de buscar la mejor manera de
salir de ella. La detección de bloqueos se ejecuta como una tarea
/periódica/, y si bien no puede prevenir situaciones de bloqueo, puede
detectarlas una vez que ya ocurrieron y limitar su impacto.

Manteniendo una lista de recursos asignados y solicitados, el sistema
operativo puede saber cuando un conjunto de procesos están esperándose
mutuamente en una solicitud por recursos — Al analizar estas tablas
como grafos dirigidos, representamos:

- Los procesos, con cuadrados

- Los recursos, con círculos

  - Puede representarse como un círculo grande a una /clase de
    recursos/, y como círculos pequeños dentro de éste a una /serie de
    recursos idénticos/ (p.ej. las diversas unidades de cinta)

- Las flechas que van de un recurso a un proceso indican que el
  recurso /está asignado/ al proceso

- Las flechas que van de un proceso a un recurso indican que el
  proceso /solicita/ al recurso

Si tenemos una representación completa de los procesos y recursos en
el sistema, la estrategia es /reducir/ la gráfica retirando los
elementos que no brinden información imprescindible, siguiendo la
siguiente lógica (recordemos que representan una fotografía del
sistema /en un momento dado/):

- Retiramos los procesos que no están solicitando ni tienen asignado
  ningún recurso.

- Para todos los procesos restantes: Si todos los recursos que están
  solicitando /pueden ser concedidos/ (esto es, no están actualmente
  asignados a otro), reducimos eliminando del grafo al proceso y a
  todas las flechas relacionadas con éste.

- Si después de esta reducción eliminamos a todos los procesos del
  grafo, entonces no hay interbloqueos y podemos continuar. En caso de
  permanecer procesos en el grafo, los procesos “irreducibles”
  constituyen la serie de procesos interbloqueados de la gráfica.

#+begin_center
#+attr_html: height="350"
#+attr_latex: width=0.5\textwidth
[[./img/deteccion_bloqueos.png]]

Detección de bloqueos: Grafo de procesos y recursos en un momento dado
#+end_center

De la gráfica anterior, podríamos proceder:

- Reducimos por /B/, dado que actualmente no está esperando a ningún
  recurso

- Reducimos por /A/ y /F/, dado que los recursos por los cuales están
  esperando quedarían libres en ausencia de /B/

Y quedamos con un interbloqueo entre /C/, /D/ y /E/, en torno a los
recursos /4/, /5/ y /7/.

#+begin_center
#+attr_html: height="350"
#+attr_latex: width=0.5\textwidth
[[./img/deteccion_bloqueos_2.png]]

Detección de bloqueos: Proceso de reducción en un grafo de procesos y
recursos, manejando clases de recursos
#+end_center

Nótese que /reducir/ un proceso del grafo no implica que éste haya
/entregado/ sus recursos, sino que únicamente que, hasta donde tenemos
conocimiento, /tiene posibilidad de hacerlo/. Los procesos que estan
esperando por recursos retenidos por un proceso pueden sufrir
inanición aún por un tiempo indeterminado.

Una vez que un bloqueo es diagnosticado, dado que los procesos no
podrán terminar por sí mismos (dado que están precisamente bloqueados,
su ejecución no avanzará más), hay varias estrategias para la
recuperación:

- Terminar a todos los procesos bloqueados. Esta es la técnica más
  sencilla y, de cierto modo, más justa — Todos los procesos
  implicados en el bloqueo pueden ser relanzados, pero todo el estado
  del cómputo que han realizado hasta este momento se perderá.

- /Retroceder/ a los procesos implicados hasta el último /punto de
  control/ (/checkpoint/) conocido. Esto es posible únicamente cuando
  el sistema implementa esta funcionalidad, que tiene un elevado costo
  adicional. Cuando el estado de uno de los procesos depende de
  factores externos a éste, es imposible implementar fielmente los
  /puntos de control/.

  Podría parecer que retroceder a un punto previo llevaría
  indefectiblemente a que se repita la situación — Pero los bloqueos
  mutuos requieren de un órden de ejecución específico para
  aparecer. Muy probablemente, dos ejecuciones posteriores lograrían
  salvar el bloqueo — y en caso contrario, puede repetirse este paso.

- Terminar, uno por uno y no en bloque, a cada uno de los procesos
  bloqueados. Una vez que se termina uno, se evalúa la situación para
  verificar si logró romperse la situación de bloqueo, en cuyo caso la
  ejecución de los restantes continúa sin interrupción.

  Para esto, si bien podría elegirse un proceso al azar de entre los
  bloqueados, típicamente se consideran elementos adicionales como:

  - Los procesos que demandan garantías de /tiempo real/ son los más
    sensibles para detener y relanzar

  - La menor cantidad de tiempo de procesador consumido hasta el
    momento. Dado que el proceso probablemente tenga que ser
    re-lanzado (re-ejecutado), puede ser conveniente /apostarle/ a un
    proceso que haya hecho poco cálculo (para que el tiempo que tenga
    que invertir para volver al punto actual sea el mínimo posible).

  - Mayor tiempo restante estimado. Si se puede estimar cuánto tiempo
    de procesamiento /queda pendiente/, conviene terminar al proceso
    que más le falte por hacer.

  - Menor número de recursos asignados hasta el momento. Un poco como
    criterio de justicia, y un poco partiendo de que es un proceso que
    está haciendo menor uso del sistema.

  - Prioridad más baja. Cuando hay un ordenamiento de procesos o
    usuarios por prioridades, siempre es preferible terminar un
    proceso de menor prioridad o perteneciente a un usuario poco
    importante que uno de mayor prioridad.

  - En caso de contar con la información necesaria, es siempre mejor
    interrumpir un proceso que /pueda ser repetido sin pérdida de
    información/ que uno que la cause. Por ejemplo, es preferible
    interrumpir una compilación que la actualización de una base de
    datos.

Un punto importante a considerar es cada cuánto debe realizarse la
verificación de bloqueos. Podría hacerse:

- Cada vez que un proceso solicite un recurso. pero esto llevaría a un
  gasto de tiempo en este análisis demasiado frecuente.

- Con una periodicidad fija, pero esto arriesga a que los procesos
  pasen más tiempo bloqueados.

- Cuando el nivel del uso del CPU baje de cierto porcentaje. Esto
  indicaría que hay un nivel elevado de procesos en espera.

- Una estrategia combinada.

Por último, si bien los dispositivos aquí mencionados requieren
bloqueo exclusivo, otra estragegia es la /apropiación temporal/: Tomar
un recurso asignado a determinado proceso para otorgárselo
/temporalmente/ a otro. Esto no siempre es posible, claro, y depende
fuertemente de la naturaleza del mismo — pero podría, por ejemplo,
interrumpirse un proceso que tiene asignada (pero inactiva) a una
impresora para otorgársela temporalmente a otro que tiene un trabajo
corto pendiente. Esto último, sin embargo, es tan sensible a detalles
de cada clase de recursos que rara vez puede hacerlo el sistema
operativo — es normalmente hecho /de acuerdo/ entre los procesos
competidores, por medio de algún protocolo pre-establecido.

** Algoritmo del banquero

Ummm... *Por desarrollar* ☹

** Algoritmo del avestruz

Una quinta línea (que, por increíble que parezca, es probablemente la
más común) es el llamado /algoritmo del avestruz/: Ignorar las
situaciones de bloqueo (escondiéndose de ellas como avestruz que
esconde la cabeza bajo la tierra), esperando que su ocurrencia sea
suficientemente poco frecuente. Hay que comprender que esto ocurre
porque las condiciones impuestas por las demás estrategias resultan
demasiado onerosas, el conocimiento previo resulta insuficiente, o los
bloqueos simplemente pueden presentarse ante recursos externos y no
controlados (o conocidos siquiera) por el sistema operativo.

Ignorar la posibilidad de un bloqueo /cuando su probabilidad es
suficientemente baja/ será preferible para los usuarios (y
programadores) ante la disyuntiva de afrontar restricciones para la
forma y conveniencia de solicitar recursos.

En este caso, se toma una decisión entre lo /correcto/ y lo
/conveniente/ — Un sistema operativo formalmente no debería permitir
la posibilidad de que hubiera bloqueos, pero la inconveniencia
presentada al usuario sería inaceptable.

Una posible salida ante la presencia del /algoritmo del avestruz/ es
que los /programadores de aplicaciones/ soliciten un recurso pero, en
vez de solicitarlo por medio de una /llamada bloqueante/, hacerlo por
medio de una /llamada no bloqueante/ y, en caso de fallar ésta,
esperar un tiempo aleatorio e intentarlo e intentar nuevamente acceder
al recurso un número dado de veces, y, tras /n/ intentos, abortar
limpiamente el proceso y notificar al usuario (evitando un bloqueo
mutuo circular indefinido).

* Otros recursos

- [[http://perldoc.perl.org/perlthrtut.html][Tutorial de hilos de Perl]]
